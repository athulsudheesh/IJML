{"title":"Your First Model","markdown":{"yaml":{"execute":{"freeze":true}},"headingText":"Your First Model","containsRefs":false,"markdown":"\n\n:::{.callout-caution icon=\"false\"}\n# In this chapter you'll learn:\n\n1. What is machine learning and what are the different types of learning algorithms.\n2. What do you mean by a model in machine learning.\n3. How to implement a simple model using `ScikitLearn.jl`\n:::\n\n## What is Machine Learning? {#sec-ml}\n\nMachine Learning is a sub-field of statistics and optimization where your goal is to design, develop, and analyze algorithms that can learn patterns in the data. Algorithms can be thought of as procedures you need to follow to achieve a goal. Some examples of instances in your life where you use an algorithm include recipes for food, instructions for the direction to a place, strategies for solving a math problem, etc. (Computer algorithms are definitely different from the above examples, but I hope you got the general gist of what an algorithm means) Let's take the example of food recipes to understand some concepts in machine learning further. \n\nEveryone who has learned to cook by themselves knows that the meal isn't guaranteed to taste that well the first time they try a new recipe. But with multiple attempts, you learn to adjust the spiciness, sourness, sweetness, gravy level, etc., to the right proportion that you will be successful in preparing an outstanding meal. If they were to record each of their attempts in a table, it would have looked something like this: \n\n```{julia}\n#| echo: false\n#| warning: false\n#| label: reiepe_data\n#| tbl-cap: \"Food ingredients and their proportions.\"\nusing Pkg\nPkg.activate\nusing DataFrames\nDataFrame(Chilly_Powder = rand(1:0.5:3,10), \n                    Sugar = Int.(rand(0:1:5,10)), \n                    Salt = rand(0:0.5:3,10),\n                    Pepper = rand(0:1:4,10),\n                    Broth_Oz = rand(3:0.5:12,10),\n                    Serves = rand(Int.(1:1:5),10),\n                    Tastes = rand([\"Edible\", \"Non-Edible\", \"Best\", \"Average\"],10) \n                    )\n```\n*Note: Values in the above table were randomly generated.*\n\nIn most cases, a table like the above one is called the data and each of your attempts (each row) is called an observation. With a data like this I can do 2 things: \n\n1. Learn how values for each of `Chilly_Powder`, `Sugar`, `Salt`, `Pepper`, `Broth_Oz` and `Serves` influence the `Tastes` and use that information to come up with the best combination of values to ensure `Best` taste all the time. This is called ***inferential modeling***. \n2. Given a set of values for `Chilly_Powder`, `Sugar`, `Salt`, `Pepper`, `Broth_Oz` and `Serves`, I can predict if the meal is going to be `Edible` or not. This is called ***predictive modeling***.\n\nIf we use the machine learning terminologies, the columns `Chilly_Powder`, `Sugar`, `Salt`, `Pepper`, `Broth_Oz` and `Serves` are called ***features*** and the column `Tastes` is called ***target***. The degree of effect each variable has on the `Tastes` are called parameters. \n\nThe mathematical representation of the above information in a functional form is called a ***model***.\nSo, for the food recipe example, our model is:\n\nChances (Probability) of the meal being edible = $f($ $\\theta_1 \\times$ `Chilly_Powder` + \n$\\theta_2 \\times$ `Sugar` +\n$\\theta_3 \\times$ `Salt` +\n$\\theta_4 \\times$ `Pepper` +\n$\\theta_5 \\times$ `Broth_Oz` +\n$\\theta_6 \\times$ `Serves` $)$\n\n### Supervised, Unsupervised, and Semi-supervised learning {#sec-super}\n\nThe parameters, $\\theta_1, \\theta_2, ....\\theta_6$, represent the patterns in the given dataset and  the goal of a Machine Learning algorithm is to find values for $\\theta_1, \\theta_2, ....\\theta_6$, so that I can reliably predict `Tastes` all the time. This type of machine learning problem, where I have information about the outcome of each attempt, is called ***supervised learning***. \n\nSuppose in our food recipe example, we didn't have information about if the meal was edible or not; finding patterns in the data is still possible. The type of machine learning problem, where I don't have information about the outcome of each attempt is called ***unsupervised learning***.  \n\nSometimes we use both supervised and unsupervised learning strategy to solve a problem and those types of machine learning problems are called ***semi-supervised learning***.\n\nNow let's learn how to implement a simple model for a supervised learning problem similar to the one we discussed above. \n\n\n## Implementing a simple model\n\nIn this section we'll learn how to implement a simple logistic regression model to predict if a woman is diabetic or not based on some of the medical information we have about that person. The dataset we are using in this section (refer @tbl-pima) is structurally similar to the food recipe example we had in the last section. Before getting into the nitty gritty details of model implementation, let's learn more about Logistic Regression.\n\n### Logistic Regression {}\n\nIn @sec-ml, we learned that a model is nothing but a mathematical representation of the relationship between the features (aka predictors) and the target. In the diabetes dataset, our target is the variable that predicts if a person is diabetic or not, and all other variables are considered features. We can represent this information in a general form as: \n\nProbability of being diabetic (i.e `Type` == 1) = \n\n$f($`NPreg`, `Glu`,`BP`, `Skin`, `BMI`, `Ped`, `Age`$) =$\n$$f(\\theta_1 \\times \\text{NPreg} +\n\\theta_2 \\times \\text{Glu} +\n\\theta_3 \\times \\text{BP} +\n\\theta_4 \\times \\text{Skin} +\n$$\n$$\n\\theta_5 \\times \\text{BMI} +\n\\theta_6 \\times \\text{Ped} +\n\\theta_7 \\times \\text{Age}) \\tag{1}$$\n\n\nIf we give a logistic parametric form to our function $f(.)$, then it's called the logistic regression model. A logistic function is defined as $$f(x) = \\frac{1}{1 + e^{-x}} \\tag{2}$$\n\nUsing equation $(2)$ on $(1)$ we get, \n\nProbability of being diabetic = \n$$\\frac{1}{1 + e^{- (\\theta_1 \\times \\text{NPreg} +\n\\theta_2 \\times \\text{Glu} +\n\\theta_3 \\times \\text{BP} +\n\\theta_4 \\times \\text{Skin} +\n\\theta_5 \\times \\text{BMI} +\n\\theta_6 \\times \\text{Ped} +\n\\theta_7 \\times \\text{Age})}} \\tag{3}$$\n\nApplying different parametric forms to equation (1) yields you different machine learning models. For e.g., if we had used an identity function i.e. $f(x) = x$, the model we get is called the linear regression model (*Note:* Linear Regression models are not used for classification problems. The type of the problem you are trying to solve always restricts the type of models you can use.). \n\nBy using an activation function where the function will return `Yes` if the value we get using equation (3) is greater than or equal to 0.50 and return `No` otherwise, we can get prediction from our model that is comparable to the target in our data. The discrepancy between our model's prediction and target is called the ***prediction error***. \n\nOnce we have a model defined and the data available, the next step is to use an algorithm to learn optimal values for $\\theta$'s so that I can predict values of the target consistently. The step where we use an algorithm to learn optimal values for $\\theta$'s is called ***model training*** and the data we used for training is called the ***training dataset*** in machine learning.\n\n### How do Models learn? {#sec-optimization}\n\nOptimization algorithms are what make model training (learning) possible. In this section, let's learn how they work from a birds-eye-view, as explaining the technicalities of how optimization algorithms work is beyond the scope of this textbook. \n\n![Schematic to understand the concept of model training.](images/logisticregression.jpg){fig.pos='h' #fig:logistic_reg}\n\nAn optimization algorithm learns pretty much the same way you learn things - through trail and error. With each trial, the goal of the optimization algorithm is to keep reducing the value of prediction error by manipulating the values for the model parameters ($\\theta$s). After several trials, we get to a point where the prediction error is in an acceptable range and reducing prediction error further is impossible or futile. At that point, we save the values of $\\theta$ that helped us to reach that particular prediction error value. These saved values for $\\theta$ are called the ***coefficients*** of our learned model and corresponds to the patterns that were present in our data. Using the learned coefficients of our model, we will be able to make predictions on data the model has never seen. The data that the model hasn't seen is called the ***test dataset*** and the prediction error we get on the test data is called the ***test error*** and the prediction error we were getting during training phase is called ***training error***.\n\n\nNow let's learn how to implement a logistic regression model and train them on the data we have \n\n### Step 1: Project environment activation and Package Installation {.unnumbered}\n\n***Note:*** We expect that you have created a separate folder for storing all the julia scripts you'll be developing as part of learning with this textbook. To open your project folder in VS Code, you can go to Menu --> File --> Open Folder. From the dialog box that pops up, you can choose the folder you created.\n\nIn order to make sure that you are always working in the correct project environment, have the following 2 lines of code towards beginning of every julia script you create:\n \n```{julia}\n#| eval: false\nusing Pkg\nPkg.activate(\".\")\n```\n   - Instructions on how to create a new julia script is described in @sec-julia-files and @sec-julia-script\n\nIn this section we will require 3 packages (To learn how to install a package, refer @sec-install-package):\n\n- `RDatasets`: This package provides an easy access to a lot of toy datasets in machine learning.\n- `ScikitLearn`: One of the industry standard packages for doing machine learning projects. Provides utilities for model definition, training, testing, tuning, and much more.\n- `DataFrames`: A package for handling data in tabular form.\n\n### Step 2: Loading the packages {.unnumbered}\nTo load these packages, you can have the following line of code right below the code you wrote in step 1:\n```{julia}\nusing ScikitLearn, RDatasets, DataFrames\n```\n\n- If you got an error while running the above line of code, most of the time it means one of the three things:\n  1. You haven't installed the package that you are trying to load.\n  2. You are in the wrong project environment. (This is why we highly recommend you to follow step 1 every time you create a new julia script.)\n  3. You have typed a wrong package name. The name of all packages in Julia are case sensitive.\n\n### Step 3: Loading the dataset {.unnumbered}\nIn this example, we will use the *Diabetes in Pima Indian Women dataset* (available via `RDatasets`). (Instruction on how to load a dataset that is available to you are a `.CSV` file is provided in the Appendix (@sec-appendix)). To load the dataset and show the first four observations, enter the following lines of code: \n```{julia}\n#| label: tbl-pima\n#| tbl-cap: \"Diabetes in Pima Indian Women dataset\"\ndiabetes = dataset(\"MASS\", \"Pima.te\");\nfirst(diabetes,4)\n```\n- `dataset` is a function from `RDatasets` that provide a nice interface to load datasets in DataFrames format. The `dataset` function accepts two arguments: the data source, and the dataset name. In this case, the name of our dataset was `Pima.te` and the source was `MASS` package in R.\n  - **Trivia:** If you see a word with `()` ending, then it is a function. A function is a collection of commands (several lines of codes) sharing a collective single objective. Anything that is passed inside `()` are called arguments. In our example, the objective of `dataset` function was to return the dataset (`Pima.te`) from the source (`MASS`) we mentioned. \n- `diabetes` is the name we gave to the variable that stores the data that was returned from the `dataset` function. The variable name is arbitrary and you can give whatever name you like. However, it is always recommended to give meaningful names.\n\n### Step 4: Making sense of the dataset {.unnumbered}\nThe diabetes dataset that we are using in this section was collected by the US National Institute of Diabetes, Digestive, and Kidney Diseases from a population of women who were 21 years and older and were of Pima Indian heritage living near Phoenix, Arizona. The dataset contains the following information:\n\n- `NPreg`: Number of pregnancies\n- `Glu`: Plasma glucose concentration in an oral glucose tolerance test\n- `BP`: Diastolic blood pressure (mm Hg)\n- `Skin`: Triceps skin fold thickness (mm)\n- `BMI`: Body Mass Index ($\\frac{\\text{weight (Kg)}}{\\text{height (m)}^2}$)\n- `Ped`: Diabetic pedigree function\n- `Age`: age in years\n- `Type`: Diabetic or not (according to WHO criteria)\n\n\n\n### Accessing elements in the data {.unnumbered}\n\nNow let's take a small detour and learn how to access different cells and slice the data.\n\n- To access the $10^\\text{th}$ row in the data:\n  \n  ```{julia}\n  #| label: tenth_row\n  #| tbl-caption: \"Tenth row in the data\"\n  diabetes[10,:]\n  ```\n  - The first position in `[]` indicated the row, and the second position indicated column. If you want to choose all columns then you put `:` in the second position.   \n\n- To access the column `BMI`:\n\n```{julia}\ndiabetes[5:9,:BMI]\n```\n  - If you want to choose all rows, then you put `!` in the first position instead of `5:9`.\n\n- To select multiple columns:\n```{julia}\n#| tbl-cap: \"Accessing multiple columns (for rows from 5:10)\"\ndiabetes[5:10,[:BMI,:Age]]\n```\n\n- To select all columns except `Type`:\n\n```{julia}\n#| tbl-cap: \"All columns except Type (for rows from 5:10)\"\ndiabetes[5:10,Not(:Type)]\n```\n\n\n### Step 5: Choosing the features and the target {.unnumbered}\n\nOur goal in this chapter is to define a supervised machine learning model that can predict if a woman is diabetic or not given their pregnancy history, glucose level, blood pressure, skin fold thickness, BMI, diabetic pedigree function, and their age. \n\nIf the value we have to predict is a category, that's called a ***classification problem *** and if the value we had to predict was numeric, it's called a ***regression problem***. Both the examples (food recipe and diabetes) we discussed in this chapter are classification problems. In the food recipe example the categories of the target were: `Non-Edible`, `Edible`, `Average`, and `Best`. For the diabetes dataset, the categories of the target are `Yes` and `No` indicating if a woman is diabetic or not.\n\nWe can use the data slicing skills we learned in the previous section to extract the features and the target from the data: \n\n```{julia}\nfeatures = Array(diabetes[!, Not(:Type)]);\ntarget = Array(diabetes[!, :Type]);\n```\n- The first line selects all columns except the `Type` column and saves them as an Array in the variable `features`\n- The second line selects just the `Type` column and saves them as an Array in the variable `target`\n\n### Step 6: Creating a Model Instance {.unnumbered}\n\nLogistic regression model is one of the most simple, common, and baseline model we use for classification problems. To create a logistic regression model instance, we can import the Logistic Regression function from `linear_models` in `ScikitLearn` package. \n\n```{julia}\n#| warning: false\n@sk_import linear_model: LogisticRegression;\nsimplelogistic = LogisticRegression();\n\n```\n- The line `simplelogistic=LogisticRegression()` creates an empty logistic regression model object which can store information about the model, data, learning algorithm, and learned parameters. The fields that are stored in a model object varies depending on the model you are defining.\n- the variable name `simplelogistic` is arbitrary and you can give whatever name you like. \n- If you are going to create another model instance (say a neural network model), don't reuse the variable name. It's better to choose a different variable name each time you are defining a new model.  \n\n### Step 7: Training your model {.unnumbered}\n\nIn @sec-optimization, you learned how an optimization algorithm helps the model to learn patterns in the data. The `fit!` function from `ScikitLearn` implements that procedure. \n```{julia}\n#| warning: false\nfit!(simplelogistic, features, target);\n```\n- the `fit!` function takes three arguments: the model you want to train, the features, and the target.\n  - Whenever you see an exclamation mark in functions, it means that the function is mutating (changing) the values of one or more arguments passed to that function. In this case `fit!` function is changing the values of $\\theta$, which is part of the model definition.\n\nNow you have a logistic regression model (`simplelogistic`) that's trained on Pima diabetes dataset. To see the learned values for $\\theta$, you can run the following line of code:\n\n```{julia}\nsimplelogistic.coef_\n```\n\nYou can plug in these values into equation (3) to reliably compute the probability of a woman being diabetic. \n\n### Code Summary for Chapter 2 {.unnumbered}\n\n```julia\n# Activating the local project environment \nusing Pkg\nPkg.activate(\".\")\n\n# Loading the packages \nusing ScikitLearn, RDatasets, DataFrames\n\n# Loading the dataset\ndiabetes = dataset(\"MASS\", \"Pima.te\");\nfirst(diabetes,4)\n\n# Choosing the features and target\nfeatures = Array(diabetes[!, Not(:Type)])\ntarget = Array(diabetes[!, :Type])\n\n# Creating a logistic regression model instance \n@sk_import linear_model: LogisticRegression\nsimplelogistic = LogisticRegression()\n\n# Training the model \nfit!(simplelogistic, features, target)\n\n# Viewing the learned parameters \nsimplelogistic.coef_\n\n```\n\nIn the next chapter, we will learn how to check if our trained model is a good one or not.\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":60000,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"firstmodel.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"0.9.342","theme":"litera","fontsize":"12pt","page-layout":"article","reader-mode":true},"extensions":{"book":{"multiFile":true}}}}}