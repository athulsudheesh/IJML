{"title":"Evaluating Your Model's Performance","markdown":{"headingText":"Evaluating Your Model's Performance","containsRefs":false,"markdown":" \n:::{.callout-caution icon=\"false\"}\n# In this chapter you'll learn:\n1. How to measure how good your model is.\n2. Different metrics to measure goodness of fit and how to use the metrics functions available in `ScikitLearn`.\n3. How to interpret the results from metrics functions.\n:::\n\n## Did our model learn anything? \nIn the last chapter, we learned how to train our simple model on the given dataset. But how do we know that our model learned the patterns in the data? Well, think about the human learning scenario. How do we come to the conclusion that somebody has learned something? \n\nThrough assessments that test their knowledge. \n\nSimilarly, we can put our model to test and see how well they perform on these tests. One of the most common and initial tests you do once you have a trained model is to check the number of times your model predicted the target value correctly, i.e., in our case, how many times our model predicted the women to be diabetic and in fact the women had diabetes as per our data records. This measure of the percentage number of times the model predicts the target value correctly is called the accuracy of the model.  \n\n$$\\text{Accuracy =} \\frac{\\text{No. of correct predictions}}{\\text{Total no. of predictions}}$$\n\nTo compute the accuracy of our model we first need to generate the predicted values for our target. This can be achieved using the predict function in `scikit-learn`.\n\n```{julia}\n#| echo: false\n#| warning: false\n# output: false \n# Activating the local project environment \nusing Pkg;\nPkg.activate(\".\");\n\n# Loading the packages \nusing ScikitLearn, RDatasets, DataFrames;\n\n# Loading the dataset\ndiabetes = dataset(\"MASS\", \"Pima.te\");\nfirst(diabetes,4);\n\n# Choosing the features and target\nfeatures = Array(diabetes[!, Not(:Type)]);\ntarget = Array(diabetes[!, :Type]);\n\n# Creating a logistic regression model instance \n@sk_import linear_model: LogisticRegression;\nsimplelogistic = LogisticRegression();\n\n# Training the model \nfit!(simplelogistic, features, target);\n```\n```{julia}\nlogistic_target_predict = predict(simplelogistic,features);\n```\n```{julia}\n#| echo: false\nfirst(logistic_target_predict,4)\n```\n- `predict` function has two arguments. The first one is the model from which you want the predictions and the second one is the data you want to predict on.\n\nOnce we have the predicted values, we can pass the predicted values and the target values from our data to the `accuracy_score` function in the `metrics` module in `scikit-learn` to compute the accuracy of our model. \n\n```{julia}\n@sk_import metrics: accuracy_score\nprint(accuracy_score(target,logistic_target_predict))\n```\n- `accuracy_score` takes two arguments: the true targets (from your given dataset) and the predicted values for target (from your model).\n```{julia}\n#| echo: false \n#| output: false\nlogistic_accuracy = accuracy_score(target,logistic_target_predict)\nlogistic_accuracy = round(logistic_accuracy, digits=4) * 100\n```\n\n```{julia}\n#| echo: false \nusing Markdown\nMarkdown.parse(\"\"\"\n- The results show that our model has an accuracy of $logistic_accuracy %.\n\"\"\")\n```\n\n## Is our model confused?\n\nAlthough accuracy is a good measure to assess the quality of your model, most often, especially in classification type problems, they don't tell us the complete story.  \n\nWhen we do prediction in a classification task, there arise four situations. For example, in our case:\n\n- the model predicted the woman to be diabetic and was in fact diabetic \n- the model predicted the woman to be non-diabetic but was diabetic \n- the model predicted the woman to be diabetic but was not actually diabetic \n- the model predicted the woman to be non-diabetic and was not diabetic\n\nThe first and last cases where our predictions aligned with the actual values are called *true positives* and *true negatives* respectively. The case where the model predicted the woman to be diabetic while she did not have diabetes is called a *false positive* case, and the case where the model predicted the woman to be non-diabetic but she was in fact diabetic is called the *false-negative* case. This is illustrated in @fig-confusion.\n\n![Four possible scenarios in the diabetes prediction task.](images/confusion.png){#fig-confusion}\n\nWhen we just focus on a classification model's accuracy, all these information is hidden from us. \n\nIt is possible to generate a figure like @fig-confusion in `scikit-learn`. For that, you need two functions, the `confusion_matrix` function to generate the confusion matrix and `ConfusionMatrixDisplay` function to generate the plot. \n\n\n```{julia}\n#| eval: false \n# Generating the confusion matrix \n@sk_import metrics: confusion_matrix\ncf = confusion_matrix(target, logistic_target_predict)\n\n# Loading the plotting library & confusion matrix plotting function\nusing PyPlot\n@sk_import metrics: ConfusionMatrixDisplay\n\nfigure() # Open a new canvas to plot\n\n# Generating the plot \ndisp = ConfusionMatrixDisplay(confusion_matrix=cf,\n            display_labels=simplelogistic.classes_)\ndisp.plot() # Transferring the plot to the canvas\ngcf() # Freezing the canvas and printing it.\n``` \n\n```{julia}\n#| echo: false \n#| warning: false\n#| fig-cap: \"Confusion Matrix\"\n#| label: fig-confusion-matrix\n# Generating the confusion matrix \n@sk_import metrics: confusion_matrix\ncf = confusion_matrix(target, logistic_target_predict)\n\n# Loading the plotting library & confusion matrix plotting function\nusing PyPlot\n@sk_import metrics: ConfusionMatrixDisplay\n\n\nfigure()\ndisp = ConfusionMatrixDisplay(confusion_matrix=cf,\n            display_labels=simplelogistic.classes_)\n#disp.plot(); \n```\n## Why our model's confusion pattern matters?\n\nYou might still be thinking why these false rates and true rates matter since we already have the accuracy scores. The importance of the confusion matrix comes into play when we consider the consequences of our prediction. For example, if it's a high consequence situation like predicting if somebody has early-stage cancer or not, miss-classifying a person as not having cancer, while they have cancer has a high cost. In such cases, the ML designer needs to look at the false negative rate more closely than the model's overall accuracy. Whereas, in a low consequence situation like credit card approval prediction, the false positive rate matters more than the false negative rate. Because, with higher false negative rate, you might be denying a credit card to a person with a good credit score and fewer chances of defaulting whereas if you have high false positive rate, you will be approving credit cards to people whose chances of defaulting are high. \n\nThe measure we are interested in the first case where the cost of missing a positive is high is called the recall or sensitivity of our model. Recall can be computed from your confusion matrix using the formulae: \n\n$$ \\text{Recall =} \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}$$\n\n```{julia}\ntrue_neg, false_neg, false_pos, true_pos = cf\nrecall = true_pos / (true_pos + false_neg)\n```\n\nThe measure we are interested in the second case, where the cost of false positive is higher is called the precision of our model. Precision can be computed using the formula:\n\n$$\\text{Precision =} \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}$$\n\n\n```{julia}\nprecision = true_pos / (true_pos + false_pos)\n```\n\nThe above results say that our model has high chances of missing a diabetes patient (due to low recall) and low chances of generating a false positive (high precision). In other words, if the model predicted you as having diabetes, there is a high chance you have diabetes, but if you were diagnosed non-diabetic by the model, you may or may not be diabetic. \n\nIf your model is used in a scenario where both false positives and false negatives have high consequences, a better metric to watch for is the F1 score. F1 score is also a better metric than accuracy score for evaluating model's performance when you have imbalances dataset (Imbalanced datasets are datasets that have unequal number of data points across its classes. For e.g., our diabetic dataset will be an imbalanced data if 90% of our data are that of non-diabetic people and only 10% is that of diabetic people.). F1 score is the weighted average of precision and recall and can be computed using the formula: \n\n$$\\text{F1 score = } 2 \\times \\frac{\\text{Recall} \\times \\text{Precision}}{\\text{Precision + Recall}}$$\n\n```{julia}\nf1 = (2 * precision * recall) / (precision + recall)\n```\n\nAlthough we had very good accuracy score, the F1 score for our model suggests that our model isn't that impressive. \n\nTo compute the recall, precision, f1 score, and accuracy with lesser lines of code, you can use the `classification_report` function from `metrics` module in `scikit-learn`.\n\n\n```{julia}\n#| warning: false\n@sk_import metrics: classification_report\nprint(classification_report(target, logistic_target_predict))\n```\n- The first line list the precision, recall, and f1 score with respect to the target \"No\" and the second with respect to the target \"Yes\". \n- In the above computations, we have calculated precision, recall, and f1 score with respect to the target \"Yes\". You can check the values we have got manually with the values printed in the classification report. \n\n## Code Summary for Chapter 3 {.unnumbered}\n\n\n```{julia}\n#| eval: false \n\n# Generating the predictions from the trained model\nlogistic_target_predict = predict(simplelogistic,features);\n\n# Calculating the accuracy score \n@sk_import metrics: accuracy_score\nprint(accuracy_score(target,logistic_target_predict))\n\n# Generating the confusion matrix \n@sk_import metrics: confusion_matrix\ncf = confusion_matrix(target, logistic_target_predict)\n\n# Loading the plotting library & confusion matrix plotting function\nusing PyPlot\n@sk_import metrics: ConfusionMatrixDisplay\n\nfigure() # Open a new canvas to plot\n\n# Generating the plot \ndisp = ConfusionMatrixDisplay(confusion_matrix=cf,\n            display_labels=simplelogistic.classes_)\ndisp.plot() # Transferring the plot to the canvas\ngcf() # Freezing the canvas and printing it.\n\n# Generating the classification report\n@sk_import metrics: classification_report\nprint(classification_report(target, logistic_target_predict))\n```\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":60000,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","include-in-header":["analytics.html"],"output-file":"evaluation.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"0.9.342","theme":"litera","fontsize":"12pt","page-layout":"article","reader-mode":true},"extensions":{"book":{"multiFile":true}}}}}