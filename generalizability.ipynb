{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generalizability\n",
        "\n",
        ":::{.callout-caution icon=\"false\"}\n",
        "# In this chapter you'll learn:\n",
        "1. What is generalizability.\n",
        "2. How to ensure generaziability of your model.\n",
        "3. What you mean by learning curve, under-fitting, over-fitting, bias and variance. \n",
        "4. How to implement cross-validation techniques using `ScikitLearn`. \n",
        "\n",
        ":::\n",
        "\n",
        "## Did our model cheat?\n",
        "\n",
        "In the last chapter, we learned how to check our model's performance using various metrics. But how do we know that our model really learned the patterns in the data and is not cheating by rote-memorizing the data? Think about how we would have assessed a human learner in this situation.\n",
        "\n",
        "\n",
        "When we want to know if students have really learned what we have asked them to learn, we test students on material that is similar to the material that's familiar to them but not exactly the questions they have seen before. Similarly, to check if our models have really learned the patterns in the data, we can test our model against a similar but unseen data. The extent to which the model performance remains invariant with this new unseen data is called that model's ***generalizability***.\n",
        "\n",
        "The portion of the data we use for training is called the ***training set*** and the portion of the data we use to test is called the ***test set***. We can use the `train_test_split` function from `model_selection` module in `scikit-learn` to create this partition.\n"
      ],
      "id": "579a6ed5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "# output: false \n",
        "# Activating the local project environment \n",
        "using Pkg;\n",
        "Pkg.activate(\".\");\n",
        "\n",
        "# Loading the packages \n",
        "using ScikitLearn, RDatasets, DataFrames;\n",
        "\n",
        "# Loading the dataset\n",
        "diabetes = dataset(\"MASS\", \"Pima.te\");\n",
        "first(diabetes,4);\n",
        "\n",
        "# Choosing the features and target\n",
        "features = Array(diabetes[!, Not(:Type)]);\n",
        "target = Array(diabetes[!, :Type]);"
      ],
      "id": "0ad28de1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| warning: false \n",
        "@sk_import model_selection: train_test_split;\n",
        "features_train, features_test, \n",
        "    target_train, target_test = train_test_split(features, \n",
        "        target, test_size=0.3, random_state=42);"
      ],
      "id": "109f6929",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The `train_test_split` function has three important and mandatory arguments. \n",
        "- The first two are the `features` and the `target`\n",
        "- The third argument is `test_size` and specifies the proportion of the data that needs to kept aside for testing. In the above code, we have asked to keep aside 30% of the data as test set.  \n",
        "- `random_state` is an optional argument and set's the seed for randomness.\n",
        "Now, instead of training the model on the entire dataset, we'll train our model with training set. \n"
      ],
      "id": "7082dfe8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| warning: false\n",
        "@sk_import linear_model: LogisticRegression;\n",
        "simplelogistic = LogisticRegression();\n",
        "\n",
        "fit!(simplelogistic, features_train, target_train);"
      ],
      "id": "96b64449",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's look at our model's performance with both training set and test set. \n",
        "\n",
        "### In-sample performance {.unnumbered}\n",
        "\n",
        "A model's performance with training set is also called it's in-sample performance. "
      ],
      "id": "50e52156"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| warning: false\n",
        "logistic_target_predict_training = \n",
        "    predict(simplelogistic,features_train);\n",
        "\n",
        "@sk_import metrics: classification_report\n",
        "print(classification_report(target_train,\n",
        "                     logistic_target_predict_training))"
      ],
      "id": "c0fd3bd1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Out-of-sample performance {.unnumbered}\n",
        "A model's performance with test set is also called it's out-of-sample performance. "
      ],
      "id": "7ab09a66"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| warning: false \n",
        "logistic_target_predict_test = \n",
        "    predict(simplelogistic,features_test);\n",
        "\n",
        "@sk_import metrics: classification_report\n",
        "print(classification_report(target_test,\n",
        "                     logistic_target_predict_test))"
      ],
      "id": "f8504d76",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By comparing our model's performance with both the training set and test set, we can see that the overall accuracy of our model slightly dropped for the test case. We can also see that our model had better precision but a little worse recall and f1-score with the test performance compared to training performance. So, we can conclude that our model has an ok-ish generalizability. \n",
        "\n",
        "## Cross-validation: A robust measure of generalizability\n",
        "We can extend the concepts of in-sample and out-of sample performance to create a more robust measure of generalizability. There are two motivations for creating this new robust measure of generalizability: \n",
        "\n",
        "1. When the sample size is small (less data), our training and test performance scores can get flaky and unreliable. \n",
        "2. There are some bells and whistles (which are called hyperparameters) that we can tweak in our models to improve our models' performance (This is explained in detail in the coming chapters). If we tweak these hyperparameters with respect to our training data, we might be overfitting our data to the training set (***Overfitting*** happens when our model have high performance on training set but very poor performance on test set.). But instead, if we tweak these hyperparameters with respect to our test data, information in the test data leaks to the model and the data is no more unseen data for the model.    \n",
        "\n",
        "ML developers came up with a solution to this problem by partitioning the training data into different data blocks, holding out one block as test set, and training on the remaining blocks. Then model's performance on the hold-out test set is saved. This process is repeated until all blocks had its chance of beginning the hold-out test set. Model performance from all these iterations is then averaged to get the robust generalization performance. This process of deriving model performance is called the ***cross-validation*** technique and is illustrated in @fig-cross-val. \n",
        "\n",
        "![Five-fold cross validation](images/cross_val.png){#fig-cross-val} \n",
        "\n",
        "To implement the K-fold cross validation technique we can use the `KFold` function and `cross_validate` function `model_selection` module in `scikit-learn`."
      ],
      "id": "c849f4fc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| warning: false\n",
        "@sk_import model_selection: KFold\n",
        "@sk_import model_selection: cross_validate\n",
        "\n",
        "cv_results = cross_validate(simplelogistic, \n",
        "        features_train, target_train, \n",
        "            cv=KFold(5),\n",
        "            return_estimator=true,\n",
        "            return_train_score=true, \n",
        "            scoring=[\"accuracy\",\n",
        "                 \"recall_weighted\", \"precision_weighted\"]);"
      ],
      "id": "ce52685c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To print the results from cross validation in a more human readable table form, we can use the following lines of code: "
      ],
      "id": "9b866c0f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| warning: false \n",
        "cv_df = DataFrame(cv_results)[!, \n",
        "        Not([:estimator, :fit_time, :score_time])]\n",
        "\n",
        "rename!(cv_df, [\"Test Accuracy\",\n",
        "                \"Test Precision\",\n",
        "                \"Test Recall\",\n",
        "                \"Train Accuracy\",\n",
        "                \"Train Precision\",\n",
        "                \"Train Recall\"])"
      ],
      "id": "39499ff2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To compute the cross validated average model performance measures, we can print the mean of each column in the above table."
      ],
      "id": "3ebd49c3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| tbl-cap: \"Cross validated average model performance measures\"\n",
        "describe(cv_df)[!,[:variable, :mean]]"
      ],
      "id": "1a3ae317",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning curves: Can more data improve model training?\n",
        "\n",
        "Now we know how to measure a model's generalization performance. But what if our models perform poorly in both training and test sets? The scenario where models fit poorly with training and test sets equally is called ***underfitting***. Underfitting usually happens due to one of the two reasons or both: a) our model is too simple for the task at hand, b) we don't have enough data to learn from. \n",
        "\n",
        "Solving the first problem is relatively simple; we can train a more complex model on the given dataset. However, solving the second problem can get complicated. Gathering more data may not always be feasible and can be expensive. In such cases, before deciding on acquiring more data, we need to make sure more data will solve the problem of underfitting. \n",
        "\n",
        "The way we figure out if more data can help with training is by training our model with data of different sizes (e.g., using 10%, 50%, and 100% of our data) and check how the model performance is varying as a function of the sample size. The plot that illustrates this relationship is called ***learning curves***.\n",
        "\n",
        "We can get our model's performance at different sample sizes using the `learning_curve` function from `model_selection` module in `scikit-learn`."
      ],
      "id": "8f0abd44"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| warning: false \n",
        "@sk_import model_selection: learning_curve;\n",
        "lc_results = learning_curve(simplelogistic, \n",
        "        features_train,target_train, \n",
        "            train_sizes = [0.06, 0.1, 0.25, 0.5, 0.75, 1.0]);"
      ],
      "id": "43d28ecc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The values we pass to `train_sizes` argument specify the different sample sizes we want to try. In this case we are looking at 10%, 25%, 50%, 75% and 100% of the data.\n"
      ],
      "id": "36f5983e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "train_sizes, train_scores, test_scores, = \n",
        "    lc_results[1], lc_results[2], lc_results[3]\n",
        "\n",
        "using Statistics\n",
        "# Calculating the error bars \n",
        "y_ax = vec(mean(test_scores, dims=2)) \n",
        "y_err = vec(std(test_scores, dims=2))\n",
        "\n",
        "using PyPlot\n",
        "begin \n",
        "    figure();\n",
        "    plot(train_sizes,\n",
        "        vec(mean(test_scores, dims=2)), label=\"Cross Val\");\n",
        "\n",
        "    fill_between(train_sizes,\n",
        "        y_ax - y_err, y_ax + y_err,alpha=0.2);\n",
        "\n",
        "    plot(train_sizes,\n",
        "        vec(mean(train_scores, dims=2)), label=\"Training\");\n",
        "    xlabel(\"Training Size\"); \n",
        "    ylabel(\"Score\"); \n",
        "    title(\"Learning Curve\");\n",
        "    legend(loc=4);\n",
        "    gcf()\n",
        "end;"
      ],
      "id": "b2e0b49e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-cap: Learning Curve for simple logistic regression model on diabetes dataset\n",
        "#| label: fig-learning-curve\n",
        "train_sizes, train_scores, test_scores, = \n",
        "    lc_results[1], lc_results[2], lc_results[3]\n",
        "\n",
        "using Statistics\n",
        "y_ax = vec(mean(test_scores, dims=2)) \n",
        "y_err = vec(std(test_scores, dims=2))\n",
        "\n",
        "using PyPlot\n",
        "begin \n",
        "    figure();\n",
        "    plot(train_sizes,\n",
        "        vec(mean(test_scores, dims=2)), label=\"Cross Val\");\n",
        "\n",
        "    fill_between(train_sizes,\n",
        "        y_ax - y_err, y_ax + y_err,alpha=0.2);\n",
        "\n",
        "    plot(train_sizes,\n",
        "        vec(mean(train_scores, dims=2)), label=\"Training\");\n",
        "    xlabel(\"Training Size (% Data)\"); \n",
        "    ylabel(\"Score\"); \n",
        "    title(\"Learning Curve\");\n",
        "    legend(loc=4);\n",
        "    ylim([0,1.09]); xlim([0,100])\n",
        "end;"
      ],
      "id": "fig-learning-curve",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the above learning curve, we see that our model's accuracy isn't getting that much influenced by increasing the sample size. So, it will be futile to collect more data for our diabetes detection ML system.\n",
        "\n",
        "### Code Summary for Chapter 4 {.unnumbered}\n"
      ],
      "id": "2f7e8acc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "\n",
        "# Creating the train-test split\n",
        "@sk_import model_selection: train_test_split;\n",
        "features_train, features_test, \n",
        "    target_train, target_test = train_test_split(features, \n",
        "        target, test_size=0.3, random_state=42);\n",
        "\n",
        "# Creating a logistic regression model instance\n",
        "@sk_import linear_model: LogisticRegression;\n",
        "simplelogistic = LogisticRegression();\n",
        "\n",
        "# Fitting the model on training data\n",
        "fit!(simplelogistic, features_train, target_train);\n",
        "\n",
        "# Generating the predictions for train data  \n",
        "logistic_target_predict_training = \n",
        "    predict(simplelogistic,features_train);\n",
        "\n",
        "# Checking the in-sample performance \n",
        "@sk_import metrics: classification_report\n",
        "print(classification_report(target_train,\n",
        "                     logistic_target_predict_training))\n",
        "\n",
        "# Generating the predictions for train data \n",
        "logistic_target_predict_test = \n",
        "    predict(simplelogistic,features_test);\n",
        "\n",
        "# Checking the out-of-sample performance\n",
        "@sk_import metrics: classification_report\n",
        "print(classification_report(target_test,\n",
        "\n",
        "# K-Fold Cross validation \n",
        "@sk_import model_selection: KFold\n",
        "@sk_import model_selection: cross_validate\n",
        "\n",
        "cv_results = cross_validate(simplelogistic, \n",
        "        features_train, target_train, \n",
        "            cv=KFold(5),\n",
        "            return_estimator=true,\n",
        "            return_train_score=true, \n",
        "            scoring=[\"accuracy\",\n",
        "                 \"recall_weighted\", \"precision_weighted\"]);\n",
        "\n",
        "# Printing cross validated results in table form \n",
        "cv_df = DataFrame(cv_results)[!, \n",
        "        Not([:estimator, :fit_time, :score_time])]\n",
        "\n",
        "rename!(cv_df, [\"Test Accuracy\",\n",
        "                \"Test Precision\",\n",
        "                \"Test Recall\",\n",
        "                \"Train Accuracy\",\n",
        "                \"Train Precision\",\n",
        "                \"Train Recall\"])\n",
        "# Cross validated means \n",
        "describe(cv_df)[!,[:variable, :mean]]\n",
        "\n",
        "# Learning curves\n",
        "@sk_import model_selection: learning_curve;\n",
        "lc_results = learning_curve(simplelogistic, \n",
        "        features_train,target_train, \n",
        "            train_sizes = [0.06, 0.1, 0.25, 0.5, 0.75, 1.0]);\n",
        "\n",
        "# Plotting learning curves\n",
        "train_sizes, train_scores, test_scores, = \n",
        "    lc_results[1], lc_results[2], lc_results[3]\n",
        "\n",
        "using Statistics\n",
        "# Calculating the error bars \n",
        "y_ax = vec(mean(test_scores, dims=2)) \n",
        "y_err = vec(std(test_scores, dims=2))\n",
        "\n",
        "using PyPlot\n",
        "begin \n",
        "    figure();\n",
        "    plot(train_sizes,\n",
        "        vec(mean(test_scores, dims=2)), label=\"Cross Val\");\n",
        "\n",
        "    fill_between(train_sizes,\n",
        "        y_ax - y_err, y_ax + y_err,alpha=0.2);\n",
        "\n",
        "    plot(train_sizes,\n",
        "        vec(mean(train_scores, dims=2)), label=\"Training\");\n",
        "    xlabel(\"Training Size\"); \n",
        "    ylabel(\"Score\"); \n",
        "    title(\"Learning Curve\");\n",
        "    legend(loc=4);\n",
        "    gcf()\n",
        "end;"
      ],
      "id": "504e813e",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "julia-1.6",
      "language": "julia",
      "display_name": "Julia 1.6.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}