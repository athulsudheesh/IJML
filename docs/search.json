[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Julia Machine Learning",
    "section": "",
    "text": "Introduction to Julia Machine Learning with Scikit-Learn is an open access book aimed at undergraduate students in non-CS majors who are taking their first course in machine learning. The book assumes very little to no prior knowledge of programming. It is designed in such a way that the book helps the students to get to speed developing machine learning models in the shortest time without diluting the fundamental concepts in ML. Although the book provides some introduction on the basics of setting up a project in Julia, it is no definitive guide to either programming or Julia Language.\nMotivations for writing this book:\n\nAt the time of writing this book, there exists a plethora of books on the ScikitLearn python library but none on the Julia port of ScikitLearn. While an experienced Julia user or someone who is new to Julia but has used python scikit-learn finds the documentation of ScikitLearn.jl complete and enough, it might be overwhelming for a complete beginner to both Julia and the ScikitLearn ecosystem. This book exists to cater to that audience.\nMost introductory books I have come across try to include as many machine learning models as they can, and in the end, they become a survey of the models and their implementation. While these books still cover the foundational concepts in a beautiful manner, they are often lost in the haystack of model details. In this book, we adopt a concepts-first approach compared to the models-first approach adopted by the vast majority of introductory textbooks on applied machine learning. The goal of this book is not to replace these existing introductory books but rather to complement and act as a prequel to them.\n\nAcknowledgement\nI would like to thank my mentor, Dr. Richard M. Golden, for training me rigorously in Statistical Machine Learning and providing me with ample opportunities to fine-tune my statistical teaching skills."
  },
  {
    "objectID": "hellojulia.html",
    "href": "hellojulia.html",
    "title": "1  Hello Julia!",
    "section": "",
    "text": "In this chapter you’ll learn:\n\n\n\n\nHow to install & setup Julia & Visual Studio Code in your computer.\nUnderstanding your Integrated Development Environment (IDE).\nHow to install packages to extend your Julia’s capabilities.\nWhat Project environments are and how to configure them."
  },
  {
    "objectID": "hellojulia.html#installation-setup",
    "href": "hellojulia.html#installation-setup",
    "title": "1  Hello Julia!",
    "section": "1.1 Installation & Setup",
    "text": "1.1 Installation & Setup\n\n\n\n\n\n\nNote\n\n\n\nThis section is written for complete beginners to programming. If you know how to configure the language extensions in VS Code, please skip this section.\n\n\n\n1.1.1 Installing Julia\nTo install the latest version of Julia, go to https://julialang.org/downloads/ and download the Current stable release corresponding to your operating system and architecture. For Windows machines, in most cases, you need to download the installer for 64-bit. For M1 Macs, it is recommended to download the Intel/Rosetta version than the M-series processor as the later version may be unstable. During the installation process, you might want to note down the installation directory for Julia (copy the path somewhere handy). This path might be required to configure your VS Code.\n\n\n1.1.2 Installing Visual Studio Code\n\n\n\n\n\n\nNote\n\n\n\nDon’t confuse Visual Studio Code (a.k.a VS Code) with Visual Studio; they are two different applications.\n\n\nYou can install the latest version of VS Code from their home page.\n\n\n1.1.3 Installing Julia Extension for VS Code\nOnce your VS Code installation is complete, you can open the application either from the Desktop (Windows) or the applications launchpad (Mac). After launching the VS Code application, there are three ways you can access the VS Code Extensions panel:\n\nvia hotkeys: Press Ctrl + Shift +X (for Windows) or Cmd + Shift +X (for Mac).\nvia menu bar: From your VS Code top menu bar choose View –> Extensions\nvia icons on the left of your VS Code application: Find and click on the icon with four squares, where one piece is detached from the rest (Figure 1.1. a).\n\n\n\n\nFigure 1.1: Installing Julia extension. Figure (a) illustrates the two ways you can access the extension panel. Figure (b) illustrates how to search and find the Julia extension.\n\n\nThis will open-up a panel towards the left side of your VS Code application; that’s your extensions panel. Towards the top of your extension panel you will see a search box where you have to search for Julia. In the results, you will see Julia listed as the top entry. Right next to it, you will also see the Install button. Click on it. (Figure 1.1. b).\n\n\n1.1.4 Checking if the Julia Extension is configured properly\nIn most cases VS Code should be able to automatically configure the path to julia executable. To check if you have got everything right:\n\nDo Ctrl + Shift + P (Windows) or Cmd + Shift + P (Mac)\nType Start Julia REPL and hit enter.\n\nIf a new panel pop-up at the bottom of your VS Code with julia prompt (as shown below), that means your VS Code Julia extension is properly configured\njulia>\n\n\n1.1.5 Fixing the Extension Configuration Manually\nIf you don’t get the julia prompt that means VS Code has failed to automatically configure the path to julia executable. To manually configure, follow these steps (also refer to Figure 1.2):\n\nOpen your VS Code extensions panel using any one of the methods mentioned above.\nSearch for Julia. Now instead of seeing the install button next to Julia entry, you will see a gear. Click on it.\nUpon clicking on the gear button, a drop down menu will pop up. Choose Extension Settings from the list. This will open up the setting page.\nIn the setting page, you should find a search box at the top. In the search box, type julia executable\nNow you will see an empty field with the title “Julia: Executable Path”.\n\n(Windows users): In this field, you need to enter/paste the path you copied during step 1 (Section 1.1.1). (Make sure the path ends with \\bin\\julia.exe).\n(Mac users): Run julia from the applications launchpad (just the way you open any other application in Mac). Once the Julia application is open you will see a path within single quotes right before the Julia logo banner. Copy everything between those quotes (excluding the quotes) and paste them into the “Julia: Executable Path” field in VS Code Julia extension.\n\n\nClose VS Code completely and open it again for the changes to reflect.\nNow repeat the steps mentioned in Section 1.1.4 to check if your configuration is working.\n\n\n\n\nFigure 1.2: Configuring Julia extension manually. Paste the path you copied during step 1 (Section 1.1.1) in the textbox area highlighted.\n\n\n\n\n1.1.6 Setting Julia as the default language in VS Code\n\nFollow the instructions described in Section 1.1.5 (until step three) to access the extension settings.\nClear all existing text in the search box of extension setting and search for default language mode.\nIn the textbox for Files: Default Language that appear, type julia (make sure all are lowercase)."
  },
  {
    "objectID": "hellojulia.html#know-your-ide",
    "href": "hellojulia.html#know-your-ide",
    "title": "1  Hello Julia!",
    "section": "1.2 Know Your IDE",
    "text": "1.2 Know Your IDE\nIDE stands for Integrated Development Environment and are software that combine developer tools into a unified user interface. The primary goal of an IDE is to increase the productivity of the developer by automating as many redundant configuration steps as possible. Modern IDEs like VS Code provide functionalities like syntax highlighting (highlights different component of language in different fonts and color), code completion (similar to word completion in MS Word), debugging (tools to debug your code when it is not behaving the way you wanted it to behave), code search (a local search engine for your project), file explorer, language terminal (for quick prototyping), and many more (Figure 1.3).\n\n\n\nFigure 1.3: Julia VS Code IDE.\n\n\nVS Code, by default, is a simple code/text editor and it is the extensions (similar to the Julia extension you installed) that bring the complete IDE experience to VS Code users. Figure 1.3 provides a visual overview of the VS Code environment during a typical Julia workflow with an active project environment. To activate the Julia language environment inside VS Code:\n\nDo Ctrl + Shift + P (Windows) or Cmd + Shift + P (Mac). This opens the Command Palette with a search box.\nType Julia REPL into the search box and hit enter.\n\nIf the Julia extension is properly configured, the above commands will open a new terminal with julia> prompt (in most cases as the bottom panel in VS Code window). This terminal with julia> prompt is commonly known as the REPL.\n\n1.2.1 Julia REPL\nREPL stands for Read-Evaluate-Print-Loop and is a method for exploratory programming and debugging. Julia’s REPL provides different prompt modes and the default one is the Julian (julia>) mode.\n\nJulian Mode. In julian mode, you can run any julia commands and the results will be displayed within the same terminal. It is very common among julia programmers to use the julian mode in the REPL to try out simple algorithms and ideas. After placing the cursor on the REPL, if you press the up arrow or down arrow, you can access your REPL history (i.e. commands you ran in the REPL).\nHelp Mode. To access help mode, first place the cursor on the REPL and press ? in your keyboard. You will see that the julia> prompt have changed to help?> prompt. This means you are in the help mode. Inside the help mode if you type a function name and hit enter, julia will attempt to print the documentation associated with that function/command in the same terminal.\nPackage Mode. Package mode can be accessed from the julian mode by pressing ] key. This will turn the julia> prompt to (@v1.7) pkg> prompt. Package mode is needed for installing, updating, and removing julia packages from your projects and computer.\n\nTo return to the default julian mode from one of the other modes, press backspace.\n\n\n1.2.2 Installing Packages\nOnce you are in the package manager mode you can install a package using the command add. For e.g., to install the StatsBase package you’ll enter add StatsBase into your package manager mode and hit enter. Once the installation of the package is over, the prompt will return back to (@v1.7) pkg. To remove a package you use the rm command, and to update a package you use update command. Just like the add command, you also need to pass the name of the package in all these cases. You can use the st command to see the list of packages installed in your computer/project environment.\n\n1.2.2.0.1 But What are Packages?\nA Julia package or a library is a collection of functions and sub-modules surrounding an idea or concept bundled as a single unit. Each function can be considered as a collection of code whose objective is to perform a specific task. The standard Julia installation comes with only a handful of very important packages to get you started. To extend the capabilities of julia, you install packages with the help of julia’s package manager (Section 1.2.2).\n\n\n\n1.2.3 Julia Files\nAlthough you can completely develop julia packages/programs/scripts within the julia REPL, an easier and faster workflow for developing code is by writing the commands in a file and running that file in the julia REPL. To open a new file, do Cmd + N (Mac) or Ctrl +N (Windows). You can also open a new file using the menu bar: Choose File –> New File. If you have configured the default language for VS Code as julia (Section 1.1.6), the newly opened file will be a julia file. It is important for VS Code to know the type of the file for syntax highlighting, auto code completion and for running the file in the appropriate language’s compiler. Once you have a file open in your VS Code, you can start writing your code line by line in that file.\n\n\n1.2.4 Running Your First Julia Script\nSuppose you wanted to write a julia script to solve for hypotenuse using the pythagoras theorem. You know that as per the pythagoras theorem, \\(c = \\sqrt{a^2 + b^2}\\), where \\(c\\) is the hypotenuse, and \\(a\\) and \\(b\\) are the sides of a right triangle. You also know that \\((3,4,5)\\) is a pythagorean triple. So now let’s implement this in code and see if it gives the right answer.\nAs a first step you enter the following lines of code into your newly opened julia file in VS Code:\n\na = 3\nb = 4\nc = sqrt(a^2 + b^2)\n\nNow save this file either using Cmd + S command or File –> Save. You can give any names you want, but it is always recommended to use meaningful names (in this case, say pythagoras.jl) so it is easier to find these files later. While saving the file also make sure the file have a .jl extension. Once you have saved the file, you have three ways to run this script:\n\nManually run the script line by line.\nRun the script as a whole.\nRun the file from REPL.\n\nTo manually run the script line by line, you can place the cursor on the first line of code and then do Shift + Enter. If the cursor hasn’t moved automatically to the next line, you can use the down arrow key to move the cursor to the next line. Now you repeat Shift+Enter until the last line of code in your file. Once a line of code is executed, the output of that command is either shown right next to the command or is printed in the REPL.\nTo run the script as a whole, you click on the play button you see in the tab bar of VS Code. While running scripts using this method, only the output of the last line of code is displayed in the REPL. In our case, the output will be:\n\n\n5.0\n\n\nTo run the file in REPL, you type include(\"YourFileName.jl\") (in our case include(\"pythagoras.jl\")) into your REPL and hit enter. The codes’ behavior will be similar to the one when you run the script as a whole using the run button."
  },
  {
    "objectID": "hellojulia.html#project-environments",
    "href": "hellojulia.html#project-environments",
    "title": "1  Hello Julia!",
    "section": "1.3 Project Environments",
    "text": "1.3 Project Environments\nA good programming practice is to always have separate folders for each Julia project you are working on. However, just having separate folders isn’t going to ensure either reproducibility or an isolated workspace. (Reproducibility is the property of your project/code to behave exactly in the same way in a computer other than one in which it was developed.). To have an isolated workspace, on top of having a separate folder for your projects, you should also be having what’s called separate project environments (Figure 1.4). Project environments are like isolated pockets of spaces where your interaction with one pocket doesn’t affect the state of another pocket.\n\n\n\nFigure 1.4: A schematic diagram to understand the concept of system environment and project environments.\n\n\nWhen you install a specific version of Julia, Julia creates an environment for that particular version. For example, when you installed Julia 1.7 in your computer, julia also created an environment with the name 1.7. This is sometimes referred as the global environment or the system environment. If you didn’t activate a particular project’s local environment before staring to work on your project, by default the system environment will be chosen and all your interaction with the package manager will be affecting the package state of your Julia’s system environment. For every project environment (including system environment), Julia creates two files: Manifest.toml and Project.toml. The goal of these files is to captures the list of packages along with their version number that you are installing within that environment.\nTo activate a local environment for your project:\n\nOpen your project folder using VS Code: File –> Open Folder.\nNow start the Julia REPL using Ctrl + Shift + P (or Cmd + Shift + P).\nEnter package manager mode. If you are seeing (@v1.7) pkg, that means you are in Julia’s system environment.\nTo create \\ activate local environment for your project, type activate . and hit Enter.\nIf your project environment was successfully activated, (@v1.7) pkg will turn into (Your Folder Name) pkg."
  },
  {
    "objectID": "firstmodel.html",
    "href": "firstmodel.html",
    "title": "2  Your First Model",
    "section": "",
    "text": "In this chapter you’ll learn:\n\n\n\n\nWhat is machine learning and what are the different types of learning algorithms.\nWhat do you mean by a model in machine learning.\nHow to implement a simple model using ScikitLearn.jl"
  },
  {
    "objectID": "firstmodel.html#sec-ml",
    "href": "firstmodel.html#sec-ml",
    "title": "2  Your First Model",
    "section": "2.1 What is Machine Learning?",
    "text": "2.1 What is Machine Learning?\nMachine Learning is a sub-field of statistics and optimization where your goal is to design, develop, and analyze algorithms that can learn patterns in the data. Algorithms can be thought of as procedures you need to follow to achieve a goal. Some examples of instances in your life where you use an algorithm include recipes for food, instructions for the direction to a place, strategies for solving a math problem, etc. (Computer algorithms are definitely different from the above examples, but I hope you got the general gist of what an algorithm means) Let’s take the example of food recipes to understand some concepts in machine learning further.\nEveryone who has learned to cook by themselves knows that the meal isn’t guaranteed to taste that well the first time they try a new recipe. But with multiple attempts, you learn to adjust the spiciness, sourness, sweetness, gravy level, etc., to the right proportion that you will be successful in preparing an outstanding meal. If they were to record each of their attempts in a table, it would have looked something like this:\n\n\n10 rows × 7 columnsFood ingredients and their proportions.Chilly_PowderSugarSaltPepperBroth_OzServesTastesFloat64Int64Float64Int64Float64Int64String13.041.527.03Edible21.543.026.51Edible33.051.5310.02Best41.012.026.52Non-Edible51.052.548.54Best62.043.0410.54Edible72.503.005.05Average82.512.504.01Average92.541.529.51Non-Edible103.051.025.02Non-Edible\n\n\nNote: Values in the above table were randomly generated.\nIn most cases, a table like the above one is called the data and each of your attempts (each row) is called an observation. With a data like this I can do 2 things:\n\nLearn how values for each of Chilly_Powder, Sugar, Salt, Pepper, Broth_Oz and Serves influence the Tastes and use that information to come up with the best combination of values to ensure Best taste all the time. This is called inferential modeling.\nGiven a set of values for Chilly_Powder, Sugar, Salt, Pepper, Broth_Oz and Serves, I can predict if the meal is going to be Edible or not. This is called predictive modeling.\n\nIf we use the machine learning terminologies, the columns Chilly_Powder, Sugar, Salt, Pepper, Broth_Oz and Serves are called features and the column Tastes is called target. The degree of effect each variable has on the Tastes are called parameters.\nThe mathematical representation of the above information in a functional form is called a model. So, for the food recipe example, our model is:\nChances (Probability) of the meal being edible = \\(f(\\) \\(\\theta_1 \\times\\) Chilly_Powder + \\(\\theta_2 \\times\\) Sugar + \\(\\theta_3 \\times\\) Salt + \\(\\theta_4 \\times\\) Pepper + \\(\\theta_5 \\times\\) Broth_Oz + \\(\\theta_6 \\times\\) Serves \\()\\)\n\n2.1.1 Supervised, Unsupervised, and Semi-supervised learning\nThe parameters, \\(\\theta_1, \\theta_2, ....\\theta_6\\), represent the patterns in the given dataset and the goal of a Machine Learning algorithm is to find values for \\(\\theta_1, \\theta_2, ....\\theta_6\\), so that I can reliably predict Tastes all the time. This type of machine learning problem, where I have information about the outcome of each attempt, is called supervised learning.\nSuppose in our food recipe example, we didn’t have information about if the meal was edible or not; finding patterns in the data is still possible. The type of machine learning problem, where I don’t have information about the outcome of each attempt is called unsupervised learning.\nSometimes we use both supervised and unsupervised learning strategy to solve a problem and those types of machine learning problems are called semi-supervised learning.\nNow let’s learn how to implement a simple model for a supervised learning problem similar to the one we discussed above."
  },
  {
    "objectID": "firstmodel.html#implementing-a-simple-model",
    "href": "firstmodel.html#implementing-a-simple-model",
    "title": "2  Your First Model",
    "section": "2.2 Implementing a simple model",
    "text": "2.2 Implementing a simple model\nIn this section we’ll learn how to implement a simple logistic regression model to predict if a woman is diabetic or not based on some of the medical information we have about that person. The dataset we are using in this section (refer Table 2.1) is structurally similar to the food recipe example we had in the last section. Before getting into the nitty gritty details of model implementation, let’s learn more about Logistic Regression.\n\n2.2.1 Logistic Regression\nIn Section 2.1, we learned that a model is nothing but a mathematical representation of the relationship between the features (aka predictors) and the target. In the diabetes dataset, our target is the variable that predicts if a person is diabetic or not, and all other variables are considered features. We can represent this information in a general form as:\nProbability of being diabetic (i.e Type == 1) =\n\\(f(\\)NPreg, Glu,BP, Skin, BMI, Ped, Age\\() =\\) \\[f(\\theta_1 \\times \\text{NPreg} +\n\\theta_2 \\times \\text{Glu} +\n\\theta_3 \\times \\text{BP} +\n\\theta_4 \\times \\text{Skin} +\n\\] \\[\n\\theta_5 \\times \\text{BMI} +\n\\theta_6 \\times \\text{Ped} +\n\\theta_7 \\times \\text{Age}) \\tag{1}\\]\nIf we give a logistic parametric form to our function \\(f(.)\\), then it’s called the logistic regression model. A logistic function is defined as \\[f(x) = \\frac{1}{1 + e^{-x}} \\tag{2}\\]\nUsing equation \\((2)\\) on \\((1)\\) we get,\nProbability of being diabetic = \\[\\frac{1}{1 + e^{- (\\theta_1 \\times \\text{NPreg} +\n\\theta_2 \\times \\text{Glu} +\n\\theta_3 \\times \\text{BP} +\n\\theta_4 \\times \\text{Skin} +\n\\theta_5 \\times \\text{BMI} +\n\\theta_6 \\times \\text{Ped} +\n\\theta_7 \\times \\text{Age})}} \\tag{3}\\]\nApplying different parametric forms to equation (1) yields you different machine learning models. For e.g., if we had used an identity function i.e. \\(f(x) = x\\), the model we got is called the linear regression model (Note: Linear Regression models are not used for classification problems. The type of the problem you are trying to solve always restricts the type of models you can use.).\nBy using an activation function where the function will return Yes if the value we get using equation (3) is greater than or equal to 0.50 and return No otherwise, we can get prediction from our model that is comparable to the target in our data. The discrepancy between our model’s prediction and target is called the prediction error.\nOnce we have a model defined and the data available, the next step is to use an algorithm to learn optimal values for \\(\\theta\\)’s so that I can predict values of the target consistently. The step where we use an algorithm to learn optimal values for \\(\\theta\\)’s is called model training and the data we used for training is called the training dataset in machine learning.\n\n\n2.2.2 How do Models learn?\nOptimization algorithms are what make model training (learning) possible. In this section, let’s learn how they work from a birds-eye-view, as explaining the technicalities of how optimization algorithms work is beyond the scope of this textbook.\n\n\n\nSchematic to understand the concept of model training.\n\n\nAn optimization algorithm learns pretty much the same way you learn things - through trail and error. With each trial, the goal of the optimization algorithm is to keep reducing the value of prediction error by manipulating the values for the model parameters (\\(\\theta\\)s). After several trials, we get to a point where the prediction error is in an acceptable range and reducing prediction error further is impossible or futile. At that point, we save the values of \\(\\theta\\) that helped us to reach that particular prediction error value. These saved values for \\(\\theta\\) are called the coefficients of our learned model and corresponds to the patterns that were present in our data. Using the learned coefficients of our model, we will be able to make predictions on data the model has never seen. The data that the model hasn’t seen is called the test dataset and the prediction error we get on the test data is called the test error and the prediction error we were getting during training phase is called training error.\nNow let’s learn how to implement a logistic regression model and train them on the data we have\n\n\nStep 1: Project environment activation and Package Installation\nNote: We expect that you have created a separate folder for storing all the julia scripts you’ll be developing as part of learning with this textbook. To open your project folder in VS Code, you can go to Menu –> File –> Open Folder. From the dialog box that pops up, you can choose the folder you created.\nIn order to make sure that you are always working in the correct project environment, have the following 2 lines of code towards beginning of every julia script you create:\n\nusing Pkg\nPkg.activate(\".\")\n\n\nInstructions on how to create a new julia script is described in Section 1.2.3 and Section 1.2.4\n\nIn this section we will require 3 packages (To learn how to install a package, refer Section 1.2.2):\n\nRDatasets: This package provides an easy access to a lot of toy datasets in machine learning.\nScikitLearn: One of the industry standard packages for doing machine learning projects. Provides utilities for model definition, training, testing, tuning, and much more.\nDataFrames: A package for handling data in tabular form.\n\n\n\nStep 2: Loading the packages\nTo load these packages, you can have the following line of code right below the code you wrote in step 1:\n\nusing ScikitLearn, RDatasets, DataFrames\n\n\nIf you got an error while running the above line of code, most of the time it means one of the three things:\n\nYou haven’t installed the package that you are trying to load.\nYou are in the wrong project environment. (This is why we highly recommend you to follow step 1 every time you create a new julia script.)\nYou have typed a wrong package name. The name of all packages in Julia are case sensitive.\n\n\n\n\nStep 3: Loading the dataset\nIn this example, we will use the Diabetes in Pima Indian Women dataset (available via RDatasets). (Instruction on how to load a dataset that is available to you are a .CSV file is provided in the Appendix (?sec-appendix)). To load the dataset and show the first four observations, enter the following lines of code:\n\ndiabetes = dataset(\"MASS\", \"Pima.te\");\nfirst(diabetes,4)\n\n\n4 rows × 8 columnsTable 2.1:  Diabetes in Pima Indian Women dataset NPregGluBPSkinBMIPedAgeTypeInt32Int32Int32Int32Float64Float64Int32Cat…16148723533.60.62750Yes2185662926.60.35131No3189662328.10.16721No4378503231.00.24826Yes\n\n\n\n\ndataset is a function from RDatasets that provide a nice interface to load datasets in DataFrames format. The dataset function accepts two arguments: the data source, and the dataset name. In this case, the name of our dataset was Pima.te and the source was MASS package in R.\n\nTrivia: If you see a word with () ending, then it is a function. A function is a collection of commands (several lines of codes) sharing a collective single objective. Anything that is passed inside () are called arguments. In our example, the objective of dataset function was to return the dataset (Pima.te) from the source (MASS) we mentioned.\n\ndiabetes is the name we gave to the variable that stores the data that was returned from the dataset function. The variable name is arbitrary and you can give whatever name you like. However, it is always recommended to give meaningful names.\n\n\n\nStep 4: Making sense of the dataset\nThe diabetes dataset that we are using in this section was collected by the US National Institute of Diabetes, Digestive, and Kidney Diseases from a population of women who were 21 years and older and were of Pima Indian heritage living near Phoenix, Arizona. The dataset contains the following information:\n\nNPreg: Number of pregnancies\nGlu: Plasma glucose concentration in an oral glucose tolerance test\nBP: Diastolic blood pressure (mm Hg)\nSkin: Triceps skin fold thickness (mm)\nBMI: Body Mass Index (\\(\\frac{\\text{weight (Kg)}}{\\text{height (m)}^2}\\))\nPed: Diabetic pedigree function\nAge: age in years\nType: Diabetic or not (according to WHO criteria)\n\n\n\nAccessing elements in the data\nNow let’s take a small detour and learn how to access different cells and slice the data.\n\nTo access the \\(10^\\text{th}\\) row in the data:\n\ndiabetes[10,:]\n\nDataFrameRow (8 columns)NPregGluBPSkinBMIPedAgeTypeInt32Int32Int32Int32Float64Float64Int32Cat…109119803529.00.26329Yes\n\n\n\nThe first position in [] indicated the row, and the second position indicated column. If you want to choose all columns then you put : in the second position.\n\nTo access the column BMI:\n\n\ndiabetes[5:9,:BMI]\n\n5-element Vector{Float64}:\n 30.5\n 25.8\n 45.8\n 43.3\n 39.3\n\n\n\nIf you want to choose all rows, then you put ! in the first position instead of 5:9.\nTo select multiple columns:\n\n\ndiabetes[5:10,[:BMI,:Age]]\n\n6 rows × 2 columnsAccessing multiple columns (for rows from 5:10)BMIAgeFloat64Int32130.553225.851345.831443.333539.327629.029\n\n\n\nTo select all columns except Type:\n\n\ndiabetes[5:10,Not(:Type)]\n\n6 rows × 7 columnsAll columns except Type (for rows from 5:10)NPregGluBPSkinBMIPedAgeInt32Int32Int32Int32Float64Float64Int3212197704530.50.1585325166721925.80.5875130118844745.80.5513141103303843.30.1833353126884139.30.7042769119803529.00.26329\n\n\n\n\nStep 5: Choosing the features and the target\nOur goal in this chapter is to define a supervised machine learning model that can predict if a woman is diabetic or not given their pregnancy history, glucose level, blood pressure, skin fold thickness, BMI, diabetic pedigree function, and their age.\nIf the value we have to predict is a category, that’s called a classification problem  and if the value we had to predict was numeric, it’s called a regression problem. Both the examples (food recipe and diabetes) we discussed in this chapter are classification problems. In the food recipe example the categories of the target were: Non-Edible, Edible, Average, and Best. For the diabetes dataset, the categories of the target are Yes and No indicating if a woman is diabetic or not.\nWe can use the data slicing skills we learned in the previous section to extract the features and the target from the data:\n\nfeatures = Array(diabetes[!, Not(:Type)]);\ntarget = Array(diabetes[!, :Type]);\n\n\nThe first line selects all columns except the Type column and saves them as an Array in the variable features\nThe second line selects just the Type column and saves them as an Array in the variable target\n\n\n\nStep 6: Creating a Model Instance\nLogistic regression model is one of the most simple, common, and baseline model we use for classification problems. To create a logistic regression model instance, we can import the Logistic Regression function from linear_models in ScikitLearn package.\n\n@sk_import linear_model: LogisticRegression;\nsimplelogistic = LogisticRegression();\n\n\nThe line simplelogistic=LogisticRegression() creates an empty logistic regression model object which can store information about the model, data, learning algorithm, and learned parameters. The fields that are stored in a model object varies depending on the model you are defining.\nthe variable name simplelogistic is arbitrary and you can give whatever name you like.\nIf you are going to create another model instance (say a neural network model), don’t reuse the variable name. It’s better to choose a different variable name each time you are defining a new model.\n\n\n\nStep 7: Training your model\nIn Section 2.2.2, you learned how an optimization algorithm helps the model to learn patterns in the data. The fit! function from ScikitLearn implements that procedure.\n\nfit!(simplelogistic, features, target);\n\n\nthe fit! function takes three arguments: the model you want to train, the features, and the target.\n\nWhenever you see an exclamation mark in functions, it means that the function is mutating (changing) the values of one or more arguments passed to that function. In this case fit! function is changing the values of \\(\\theta\\), which is part of the model definition.\n\n\nNow you have a logistic regression model (simplelogistic) that’s trained on Pima diabetes dataset. To see the learned values for \\(\\theta\\), you can run the following line of code:\n\nsimplelogistic.coef_\n\n1×7 Matrix{Float64}:\n 0.138633  0.0373939  -0.00897535  0.0134173  0.0783658  0.921752  0.0190434\n\n\nYou can plug in these values into equation (3) to reliably compute the probability of a woman being diabetic.\n\n\nCode Summary for Chapter 2\n# Activating the local project environment \nusing Pkg\nPkg.activate(\".\")\n\n# Loading the packages \nusing ScikitLearn, RDatasets, DataFrames\n\n# Loading the dataset\ndiabetes = dataset(\"MASS\", \"Pima.te\");\nfirst(diabetes,4)\n\n# Choosing the features and target\nfeatures = Array(diabetes[!, Not(:Type)])\ntarget = Array(diabetes[!, :Type])\n\n# Creating a logistic regression model instance \n@sk_import linear_model: LogisticRegression\nsimplelogistic = LogisticRegression()\n\n# Training the model \nfit!(simplelogistic, features, target)\n\n# Viewing the learned parameters \nsimplelogistic.coef_\nIn the next chapter, we will learn how to check if our trained model is a good one or not."
  },
  {
    "objectID": "evaluation.html",
    "href": "evaluation.html",
    "title": "3  Evaluating Your Model’s Performance",
    "section": "",
    "text": "In this chapter you’ll learn:\n\n\n\n\nHow to measure how good your model is.\nDifferent metrics to measure goodness of fit and how to use the metrics functions available in ScikitLearn.\nHow to interpret the results from metrics functions."
  },
  {
    "objectID": "evaluation.html#did-our-model-learn-anything",
    "href": "evaluation.html#did-our-model-learn-anything",
    "title": "3  Evaluating Your Model’s Performance",
    "section": "3.1 Did our model learn anything?",
    "text": "3.1 Did our model learn anything?\nIn the last chapter, we learned how to train our simple model on the given dataset. But how do we know that our model learned the patterns in the data? Well, think about the human learning scenario. How do we come to the conclusion that somebody has learned something?\nThrough assessments that test their knowledge.\nSimilarly, we can put our model to test and see how well they perform on these tests. One of the most common and initial tests you do once you have a trained model is to check the number of times your model predicted the target value correctly, i.e., in our case, how many times our model predicted the women to be diabetic and in fact the women had diabetes as per our data records. This measure of the percentage number of times the model predicts the target value correctly is called the accuracy of the model.\n\\[\\text{Accuracy =} \\frac{\\text{No. of correct predictions}}{\\text{Total no. of predictions}}\\]\nTo compute the accuracy of our model we first need to generate the predicted values for our target. This can be achieved using the predict function in scikit-learn.\n\nlogistic_target_predict = predict(simplelogistic,features);\n\n\n\n4-element Vector{Any}:\n \"Yes\"\n \"No\"\n \"No\"\n \"No\"\n\n\nOnce we have the predicted values, we can pass the predicted values and the target values from our data to the accuracy_score function in the metrics module in scikit-learn to compute the accuracy of our model.\n\n@sk_import metrics: accuracy_score\nprint(accuracy_score(target,logistic_target_predict))\n\n0.7921686746987951\n\n\n\n\n\nThe results show that our model has an accuracy of 79.22 %."
  },
  {
    "objectID": "evaluation.html#is-our-model-confused",
    "href": "evaluation.html#is-our-model-confused",
    "title": "3  Evaluating Your Model’s Performance",
    "section": "3.2 Is our model confused?",
    "text": "3.2 Is our model confused?\nAlthough accuracy is a good measure to assess the quality of your model, most often, especially in classification type problems, they don’t tell us the complete story.\nWhen we do prediction in a classification task, there arise four situations. For example, in our case:\n\nthe model predicted the woman to be diabetic and was in fact diabetic\nthe model predicted the woman to be non-diabetic but was diabetic\nthe model predicted the woman to be diabetic but was not actually diabetic\nthe model predicted the woman to be non-diabetic and was not diabetic\n\nThe first and last cases where our predictions aligned with the actual values are called true positives and true negatives respectively. The case where the model predicted the woman to be diabetic while she did not have diabetes is called a false positive case, and the case where the model predicted the woman to be non-diabetic but she was in fact diabetic is called the false-negative case. This is illustrated in Figure 3.1.\n\n\n\nFigure 3.1: Four possible scenarios in the diabetes prediction task.\n\n\nWhen we just focus on a classification model’s accuracy, all these information is hidden from us.\nIt is possible to generate a figure like Figure 3.1 in scikit-learn. For that, you need two functions, the confusion_matrix function to generate the confusion matrix and ConfusionMatrixDisplay function to generate the plot.\n\n# Generating the confusion matrix \n@sk_import metrics: confusion_matrix\ncf = confusion_matrix(target, logistic_target_predict)\n\n# Loading the plotting library & confusion matrix plotting function\nusing PyPlot\n@sk_import metrics: ConfusionMatrixDisplay\n\nfigure() # Open a new canvas to plot\n\n# Generating the plot \ndisp = ConfusionMatrixDisplay(confusion_matrix=cf,display_labels=simplelogistic.classes_)\ndisp.plot() # Transferring the plot to the canvas\ngcf() # Freezing the canvas and printing it.\n\n\n\n\n\n\nFigure 3.2: Confusion Matrix"
  },
  {
    "objectID": "evaluation.html#why-our-models-confusion-pattern-matters",
    "href": "evaluation.html#why-our-models-confusion-pattern-matters",
    "title": "3  Evaluating Your Model’s Performance",
    "section": "3.3 Why our model’s confusion pattern matters?",
    "text": "3.3 Why our model’s confusion pattern matters?\nYou might still be thinking why these false rates and true rates matter since we already have the accuracy scores. The importance of the confusion matrix comes into play when we consider the consequences of our prediction. For example, if it’s a high consequence situation like predicting if somebody has early-stage cancer or not, miss-classifying a person as not having cancer, while they have cancer has a high cost. In such cases, the ML designer needs to look at the false negative rate more closely than the model’s overall accuracy. Whereas, in a low consequence situation like credit card approval prediction, the false positive rate matters more than the false negative rate. Because, with higher false negative rate, you might be denying a credit card to a person with a good credit score and fewer chances of defaulting whereas if you have high false positive rate, you will be approving credit cards to people whose chances of defaulting are high.\nThe measure we are interested in the first case where the cost of missing a positive is high is called the recall or sensitivity of our model. Recall can be computed from your confusion matrix using the formulae:\n\\[ \\text{Recall =} \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}\\]\n\ntrue_neg, false_neg, false_pos, true_pos = cf\nrecall = true_pos / (true_pos + false_neg)\n\n0.5688073394495413\n\n\nThe measure we are interested in the second case, where the cost of false positive is higher is called the precision of our model. Precision can be computed using the formula:\n\\[\\text{Precision =} \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}\\]\n\nprecision = true_pos / (true_pos + false_pos)\n\n0.7380952380952381\n\n\nThe above results say that our model has high chances of missing a diabetes patient (due to low recall) and low chances of generating a false positive (high precision). In other words, if the model predicted you as having diabetes, there is a high chance you have diabetes, but if you were diagnosed non-diabetic by the model, you may or may not be diabetic.\nIf your model is used in a scenario where both false positives and false negatives have high consequences, a better metric to watch for is the F1 score. F1 score is also a better metric than accuracy score for evaluating model’s performance when you have imbalances dataset (Imbalanced datasets are datasets that have unequal number of data points across its classes. For e.g., our diabetic dataset will be an imbalanced data if 90% of our data are that of non-diabetic people and only 10% is that of diabetic people.). F1 score is the weighted average of precision and recall and can be computed using the formula:\n\\[\\text{F1 score = } 2 \\times \\frac{\\text{Recall} \\times \\text{Precision}}{\\text{Precision + Recall}}\\]\n\nf1 = (2 * precision * recall) / (precision + recall)\n\n0.6424870466321243\n\n\nAlthough we had very good accuracy score, the F1 score for our model suggests that our model isn’t that impressive.\nTo compute the recall, precision, f1 score, and accuracy with lesser lines of code, you can use the classification_report function from metrics module in scikit-learn.\n\n@sk_import metrics: classification_report\nprint(classification_report(target, logistic_target_predict))\n\n              precision    recall  f1-score   support\n\n          No       0.81      0.90      0.85       223\n         Yes       0.74      0.57      0.64       109\n\n    accuracy                           0.79       332\n   macro avg       0.77      0.74      0.75       332\nweighted avg       0.79      0.79      0.78       332\n\n\n\nThe first line list the precision, recall, and f1 score with respect to the target “No” and the second with respect to the target “Yes”.\nIn the above computations, we have calculated precision, recall, and f1 score with respect to the target “Yes”. You can check the values we have got manually with the values printed in the classification report."
  },
  {
    "objectID": "evaluation.html#code-summary-for-chapter-3",
    "href": "evaluation.html#code-summary-for-chapter-3",
    "title": "3  Evaluating Your Model’s Performance",
    "section": "Code Summary for Chapter 3",
    "text": "Code Summary for Chapter 3\n\n# Generating the predictions from the trained model\nlogistic_target_predict = predict(simplelogistic,features);\n\n# Calculating the accuracy score \n@sk_import metrics: accuracy_score\nprint(accuracy_score(target,logistic_target_predict))\n\n# Generating the confusion matrix \n@sk_import metrics: confusion_matrix\ncf = confusion_matrix(target, logistic_target_predict)\n\n# Loading the plotting library & confusion matrix plotting function\nusing PyPlot\n@sk_import metrics: ConfusionMatrixDisplay\n\nfigure() # Open a new canvas to plot\n\n# Generating the plot \ndisp = ConfusionMatrixDisplay(confusion_matrix=cf,\n            display_labels=simplelogistic.classes_)\ndisp.plot() # Transferring the plot to the canvas\ngcf() # Freezing the canvas and printing it.\n\n# Generating the classification report\n@sk_import metrics: classification_report\nprint(classification_report(target, logistic_target_predict))"
  },
  {
    "objectID": "generalizability.html",
    "href": "generalizability.html",
    "title": "4  Generalizability",
    "section": "",
    "text": "In this chapter you’ll learn:\n\n\n\n\nWhat is generalizability.\nHow to ensure generaziability of your model.\nWhat you mean by learning curve, under-fitting, over-fitting, bias and variance.\nHow to implement cross-validation techniques using ScikitLearn."
  },
  {
    "objectID": "generalizability.html#did-our-model-cheat",
    "href": "generalizability.html#did-our-model-cheat",
    "title": "4  Generalizability",
    "section": "4.1 Did our model cheat?",
    "text": "4.1 Did our model cheat?\nIn the last chapter, we learned how to check our model’s performance using various metrics. But how do we know that our model really learned the patterns in the data and is not cheating by rote-memorizing the data? Think about how we would have assessed a human learner in this situation.\nWhen we want to know if students have really learned what we have asked them to learn, we test students on material that is similar to the material that’s familiar to them but not exactly the questions they have seen before. Similarly, to check if our models have really learned the patterns in the data, we can test our model against a similar but unseen data. The extent to which the model performance remains invariant with this new unseen data is called that model’s generalizability.\nThe portion of the data we use for training is called the training set and the portion of the data we use to test is called the test set. We can use the train_test_split function from model_selection module in scikit-learn to create this partition.\n\n@sk_import model_selection: train_test_split;\nfeatures_train, features_test, \n    target_train, target_test = train_test_split(features, \n        target, test_size=0.3, random_state=42);\n\n\nThe train_test_split function has three important and mandatory arguments.\nThe first two are the features and the target\nThe third argument is test_size and specifies the proportion of the data that needs to kept aside for testing. In the above code, we have asked to keep aside 30% of the data as test set.\n\nrandom_state is an optional argument and set’s the seed for randomness. Now, instead of training the model on the entire dataset, we’ll train our model with training set.\n\n\n@sk_import linear_model: LogisticRegression;\nsimplelogistic = LogisticRegression();\n\nfit!(simplelogistic, features_train, target_train);\n\nNow let’s look at our model’s performance with both training set and test set.\n\nIn-sample performance\nA model’s performance with training set is also called it’s in-sample performance.\n\nlogistic_target_predict_training = \n    predict(simplelogistic,features_train);\n\n@sk_import metrics: classification_report\nprint(classification_report(target_train,\n                     logistic_target_predict_training))\n\n              precision    recall  f1-score   support\n\n          No       0.84      0.91      0.87       166\n         Yes       0.71      0.56      0.63        66\n\n    accuracy                           0.81       232\n   macro avg       0.78      0.74      0.75       232\nweighted avg       0.80      0.81      0.80       232\n\n\n\n\nOut-of-sample performance\nA model’s performance with test set is also called it’s out-of-sample performance.\n\nlogistic_target_predict_test = \n    predict(simplelogistic,features_test);\n\n@sk_import metrics: classification_report\nprint(classification_report(target_test,\n                     logistic_target_predict_test))\n\n              precision    recall  f1-score   support\n\n          No       0.72      0.98      0.83        57\n         Yes       0.95      0.49      0.65        43\n\n    accuracy                           0.77       100\n   macro avg       0.84      0.74      0.74       100\nweighted avg       0.82      0.77      0.75       100\n\n\nBy comparing our model’s performance with both the training set and test set, we can see that the overall accuracy of our model slightly dropped for the test case. We can also see that our model had better precision but a little worse recall and f1-score with the test performance compared to training performance. So, we can conclude that our model has an ok-ish generalizability."
  },
  {
    "objectID": "generalizability.html#cross-validation-a-robust-measure-of-generalizability",
    "href": "generalizability.html#cross-validation-a-robust-measure-of-generalizability",
    "title": "4  Generalizability",
    "section": "4.2 Cross-validation: A robust measure of generalizability",
    "text": "4.2 Cross-validation: A robust measure of generalizability\nWe can extend the concepts of in-sample and out-of sample performance to create a more robust measure of generalizability. There are two motivations for creating this new robust measure of generalizability:\n\nWhen the sample size is small (less data), our training and test performance scores can get flaky and unreliable.\nThere are some bells and whistles (which are called hyperparameters) that we can tweak in our models to improve our models’ performance (This is explained in detail in the coming chapters). If we tweak these hyperparameters with respect to our training data, we might be overfitting our data to the training set (Overfitting happens when our model have high performance on training set but very poor performance on test set.). But instead, if we tweak these hyperparameters with respect to our test data, information in the test data leaks to the model and the data is no more unseen data for the model.\n\nML developers came up with a solution to this problem by partitioning the training data into different data blocks, holding out one block as test set, and training on the remaining blocks. Then model’s performance on the hold-out test set is saved. This process is repeated until all blocks had its chance of beginning the hold-out test set. Model performance from all these iterations is then averaged to get the robust generalization performance. This process of deriving model performance is called the cross-validation technique and is illustrated in Figure 4.1.\n\n\n\nFigure 4.1: Five-fold cross validation\n\n\nTo implement the K-fold cross validation technique we can use the KFold function and cross_validate function model_selection module in scikit-learn.\n\n@sk_import model_selection: KFold\n@sk_import model_selection: cross_validate\n\ncv_results = cross_validate(simplelogistic, \n        features_train, target_train, \n            cv=KFold(5),\n            return_estimator=true,\n            return_train_score=true, \n            scoring=[\"accuracy\",\n                 \"recall_weighted\", \"precision_weighted\"]);\n\nTo print the results from cross validation in a more human readable table form, we can use the following lines of code:\n\ncv_df = DataFrame(cv_results)[!, \n        Not([:estimator, :fit_time, :score_time])]\n\nrename!(cv_df, [\"Test Accuracy\",\n                \"Test Precision\",\n                \"Test Recall\",\n                \"Train Accuracy\",\n                \"Train Precision\",\n                \"Train Recall\"])\n\n5 rows × 6 columnsTest AccuracyTest PrecisionTest RecallTrain AccuracyTrain PrecisionTrain RecallFloat64Float64Float64Float64Float64Float6410.8297870.8182370.8297870.8378380.8338540.83783820.8085110.8010090.8085110.80.7924560.830.760870.7429920.760870.8118280.8040970.81182840.8478260.8519990.8478260.8118280.8050280.81182850.7826090.7826090.7826090.8172040.8085550.817204\n\n\nTo compute the cross validated average model performance measures, we can print the mean of each column in the above table.\n\ndescribe(cv_df)[!,[:variable, :mean]]\n\n6 rows × 2 columnsCross validated average model performance measuresvariablemeanSymbolFloat641Test Accuracy0.805922Test Precision0.7993693Test Recall0.805924Train Accuracy0.815745Train Precision0.8087986Train Recall0.81574"
  },
  {
    "objectID": "generalizability.html#learning-curves-can-more-data-improve-model-training",
    "href": "generalizability.html#learning-curves-can-more-data-improve-model-training",
    "title": "4  Generalizability",
    "section": "4.3 Learning curves: Can more data improve model training?",
    "text": "4.3 Learning curves: Can more data improve model training?\nNow we know how to measure a model’s generalization performance. But what if our models perform poorly in both training and test sets? The scenario where models fit poorly with training and test sets equally is called underfitting. Underfitting usually happens due to one of the two reasons or both: a) our model is too simple for the task at hand, b) we don’t have enough data to learn from.\nSolving the first problem is relatively simple; we can train a more complex model on the given dataset. However, solving the second problem can get complicated. Gathering more data may not always be feasible and can be expensive. In such cases, before deciding on acquiring more data, we need to make sure more data will solve the problem of underfitting.\nThe way we figure out if more data can help with training is by training our model with data of different sizes (e.g., using 10%, 50%, and 100% of our data) and check how the model performance is varying as a function of the sample size. The plot that illustrates this relationship is called learning curves.\nWe can get our model’s performance at different sample sizes using the learning_curve function from model_selection module in scikit-learn.\n\n@sk_import model_selection: learning_curve;\nlc_results = learning_curve(simplelogistic, \n        features_train,target_train, \n            train_sizes = [0.06, 0.1, 0.25, 0.5, 0.75, 1.0]);\n\n\nThe values we pass to train_sizes argument specify the different sample sizes we want to try. In this case we are looking at 10%, 25%, 50%, 75% and 100% of the data.\n\n\ntrain_sizes, train_scores, test_scores, = \n    lc_results[1], lc_results[2], lc_results[3]\n\nusing Statistics\n# Calculating the error bars \ny_ax = vec(mean(test_scores, dims=2)) \ny_err = vec(std(test_scores, dims=2))\n\nusing PyPlot\nbegin \n    figure();\n    plot(train_sizes,\n        vec(mean(test_scores, dims=2)), label=\"Cross Val\");\n\n    fill_between(train_sizes,\n        y_ax - y_err, y_ax + y_err,alpha=0.2);\n\n    plot(train_sizes,\n        vec(mean(train_scores, dims=2)), label=\"Training\");\n    xlabel(\"Training Size\"); \n    ylabel(\"Score\"); \n    title(\"Learning Curve\");\n    legend(loc=4);\n    gcf()\nend;\n\n\n\n\n\n\nFigure 4.2: Learning Curve for simple logistic regression model on diabetes dataset\n\n\n\n\nFrom the above learning curve, we see that our model’s accuracy isn’t getting that much influenced by increasing the sample size. So, it will be futile to collect more data for our diabetes detection ML system.\n\nCode Summary for Chapter 4\n\n# Creating the train-test split\n@sk_import model_selection: train_test_split;\nfeatures_train, features_test, \n    target_train, target_test = train_test_split(features, \n        target, test_size=0.3, random_state=42);\n\n# Creating a logistic regression model instance\n@sk_import linear_model: LogisticRegression;\nsimplelogistic = LogisticRegression();\n\n# Fitting the model on training data\nfit!(simplelogistic, features_train, target_train);\n\n# Generating the predictions for train data  \nlogistic_target_predict_training = \n    predict(simplelogistic,features_train);\n\n# Checking the in-sample performance \n@sk_import metrics: classification_report\nprint(classification_report(target_train,\n                     logistic_target_predict_training))\n\n# Generating the predictions for train data \nlogistic_target_predict_test = \n    predict(simplelogistic,features_test);\n\n# Checking the out-of-sample performance\n@sk_import metrics: classification_report\nprint(classification_report(target_test,\n\n# K-Fold Cross validation \n@sk_import model_selection: KFold\n@sk_import model_selection: cross_validate\n\ncv_results = cross_validate(simplelogistic, \n        features_train, target_train, \n            cv=KFold(5),\n            return_estimator=true,\n            return_train_score=true, \n            scoring=[\"accuracy\",\n                 \"recall_weighted\", \"precision_weighted\"]);\n\n# Printing cross validated results in table form \ncv_df = DataFrame(cv_results)[!, \n        Not([:estimator, :fit_time, :score_time])]\n\nrename!(cv_df, [\"Test Accuracy\",\n                \"Test Precision\",\n                \"Test Recall\",\n                \"Train Accuracy\",\n                \"Train Precision\",\n                \"Train Recall\"])\n# Cross validated means \ndescribe(cv_df)[!,[:variable, :mean]]\n\n# Learning curves\n@sk_import model_selection: learning_curve;\nlc_results = learning_curve(simplelogistic, \n        features_train,target_train, \n            train_sizes = [0.06, 0.1, 0.25, 0.5, 0.75, 1.0]);\n\n# Plotting learning curves\ntrain_sizes, train_scores, test_scores, = \n    lc_results[1], lc_results[2], lc_results[3]\n\nusing Statistics\n# Calculating the error bars \ny_ax = vec(mean(test_scores, dims=2)) \ny_err = vec(std(test_scores, dims=2))\n\nusing PyPlot\nbegin \n    figure();\n    plot(train_sizes,\n        vec(mean(test_scores, dims=2)), label=\"Cross Val\");\n\n    fill_between(train_sizes,\n        y_ax - y_err, y_ax + y_err,alpha=0.2);\n\n    plot(train_sizes,\n        vec(mean(train_scores, dims=2)), label=\"Training\");\n    xlabel(\"Training Size\"); \n    ylabel(\"Score\"); \n    title(\"Learning Curve\");\n    legend(loc=4);\n    gcf()\nend;"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "5  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  }
]