{
  "hash": "faaa39c5e8198a11842c4761a01e7bfb",
  "result": {
    "markdown": "---\nexecute:\n  freeze: true\n---\n\n# The End-to-end Workflow: A recap\n\n:::{.callout-caution icon=\"false\"}\n# In this chapter:\nWe will do a recap of everything we have learned until now.\n:::\n\n## I. Environment Setup  {.unnumbered}\n\n### Step 1: Open the **Project** folder in VS Code  {.unnumbered}\n\nGo to File --> Open Folder.\n\n### Step 2: Start Julia REPL  {.unnumbered}\n\nPress `Cmd` + `Shift` + `P` (Mac) / `Ctrl` + `Shift` + `P` (Windows), and then search for `Julia: Start REPL`. Hit enter to choose the option from the dropdown menu \n\n### Step 3: Activate Project Environment  {.unnumbered}\n\n\n- Go to package manager mode. (Press `]` while in Julia REPL to go to Package Manager mode). Then type `activate .` and hit enter. Now you should see your project folder name instead of `(@v1.7) pkg>`. \n- Return to Julia REPL mode by hitting backspace \n\n## Step 4: Create new Julia file for your project  {.unnumbered}\n\n- Go to File --> New File \n- Save the file with a name you like. To save the file you can do `Cmd` + `S` / `Ctrl` + `S`.\n\n### Step 5: Make sure you have the required packages {.unnumbered}\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\nusing Pkg\nPkg.status()\n```\n:::\n\n\n- CSV\n- DataFrames\n- PyPlot\n- ScikitLearn\n\nIf some packages are missing, you'll get an error in the next step.\n\n### Step 6: Load the packages  {.unnumbered}\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\nusing DataFrames\nusing CSV\nusing ScikitLearn\nusing PyPlot\n```\n:::\n\n\n## II. Data  {.unnumbered}\n\n### Step 7: Load the Data {.unnumbered}\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\npathtodata = joinpath(pwd(), \"diabetes.csv\")\ndata = CSV.read(pathtodata, DataFrame);\n```\n:::\n\n\n- Instead of `diabetes.csv`, pass your dataset name.\n\n### Step 8: Inspect the data columns  {.unnumbered}\n\n::: {.cell execution_count=5}\n``` {.julia .cell-code}\nfirst(data,5)\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div class=\"data-frame\"><p>5 rows × 8 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>npreg</th><th>glu</th><th>bp</th><th>skin</th><th>bmi</th><th>ped</th><th>age</th><th>type</th></tr><tr><th></th><th title=\"Int64\">Int64</th><th title=\"Int64\">Int64</th><th title=\"Int64\">Int64</th><th title=\"Int64\">Int64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Int64\">Int64</th><th title=\"String3\">String3</th></tr></thead><tbody><tr><th>1</th><td>6</td><td>148</td><td>72</td><td>35</td><td>33.6</td><td>0.627</td><td>50</td><td>Yes</td></tr><tr><th>2</th><td>1</td><td>85</td><td>66</td><td>29</td><td>26.6</td><td>0.351</td><td>31</td><td>No</td></tr><tr><th>3</th><td>1</td><td>89</td><td>66</td><td>23</td><td>28.1</td><td>0.167</td><td>21</td><td>No</td></tr><tr><th>4</th><td>3</td><td>78</td><td>50</td><td>32</td><td>31.0</td><td>0.248</td><td>26</td><td>Yes</td></tr><tr><th>5</th><td>2</td><td>197</td><td>70</td><td>45</td><td>30.5</td><td>0.158</td><td>53</td><td>Yes</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n- If you find columns that are redundant or meaningless, delete those columns. For example, suppose we had a meaningless column called `RowNo`. To delete the column you can use the command `data = select(data,Not(:RowNo))`. Make sure you only run the delete command once per column. Trying to run it multiple times will give you an error. \n\n\n### Step 9: Extract the features and target  {.unnumbered}\n\n::: {.cell execution_count=6}\n``` {.julia .cell-code}\nX = Array(data[!,Not(:type)]);\ny = Array(data[!, :type]);\n```\n:::\n\n\n- Instead of `:type` in the above code, you should use the column name you have for your target.\n\n\n## III. Model Training  {.unnumbered}\n\n### Step 10: Split the data into training set and test set.  {.unnumbered}\n\n::: {.cell freeze='true' execution_count=7}\n``` {.julia .cell-code}\n@sk_import model_selection: train_test_split;\nX_train, X_test, Y_train, Y_test = \n    train_test_split(X, y, test_size=0.2, \n        random_state=42);\n```\n:::\n\n\n- Here I chose 20% for test set (so 80% of the actual data will be used for training).\n\n### Step 11: Build a baseline model  {.unnumbered}\n\nBefore going into complicated models, it's recommended that you try a simple model first. You'll call this your baseline model. In our case, we'll have a simple logistic regression model our baseline. \n\n::: {.cell freeze='true' execution_count=8}\n``` {.julia .cell-code}\n@sk_import linear_model: LogisticRegression;\nsimplelogistic =LogisticRegression();\n\nfit!(simplelogistic, X_train, Y_train);\n```\n:::\n\n\n### Step 12: Check the accuracy of your baseline model  {.unnumbered}\n\n::: {.cell execution_count=9}\n``` {.julia .cell-code}\n@sk_import metrics: accuracy_score\n\nY_pred_train = predict(simplelogistic,X_train);\nprint(accuracy_score(Y_train,Y_pred_train))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.7886792452830189\n```\n:::\n:::\n\n\n### Step 13: Create a slightly complicated model and check its accuracy  {.unnumbered}\n\nWe'll have a shallow neural network with one hidden layer and 5 nodes as our slightly complicated model. \n\n::: {.cell freeze='true' execution_count=10}\n``` {.julia .cell-code}\n@sk_import neural_network: MLPClassifier;\nsimpleneuralnetwork = MLPClassifier(hidden_layer_sizes=(5));\n\nfit!(simpleneuralnetwork, X_train, Y_train);\n\nY_pred_train = predict(simpleneuralnetwork,X_train);\nprint(accuracy_score(Y_train,Y_pred_train))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.30943396226415093\n```\n:::\n:::\n\n\n### Step 14: Search the model space for better hyperparameters with 3-Fold cross validated training  {.unnumbered}\n\n::: {.cell freeze='true' execution_count=11}\n``` {.julia .cell-code}\nusing ScikitLearn.GridSearch: GridSearchCV\ngridsearch_logistic = GridSearchCV(LogisticRegression(),\n            Dict(:solver => [\"newton-cg\", \"lbfgs\", \"liblinear\"], \n            :C => [0.01, 0.1, 0.5, 0.9]))\n\nfit!(gridsearch_logistic, X_train, Y_train);\n```\n:::\n\n\n- `:C` is a regularization parameter. \n- For more hyperparameters that are tweakable, please refer to LogisticRegression's ScikitLearn documentation page. \n\n#### Print the results of hyperparameter search  {.unnumbered}\n\n::: {.cell execution_count=12}\n``` {.julia .cell-code}\ngridsearch_logistic_results = DataFrame(gridsearch_logistic.grid_scores_);\nhcat(DataFrame(gridsearch_logistic_results.parameters), \n    gridsearch_logistic_results)[!,Not(:parameters)]\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div class=\"data-frame\"><p>12 rows × 4 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>solver</th><th>C</th><th>mean_validation_score</th><th>cv_validation_scores</th></tr><tr><th></th><th title=\"String\">String</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Vector{Float64}\">Array…</th></tr></thead><tbody><tr><th>1</th><td>newton-cg</td><td>0.01</td><td>0.8</td><td>[0.764045, 0.829545, 0.806818]</td></tr><tr><th>2</th><td>lbfgs</td><td>0.01</td><td>0.8</td><td>[0.764045, 0.829545, 0.806818]</td></tr><tr><th>3</th><td>liblinear</td><td>0.01</td><td>0.720755</td><td>[0.696629, 0.715909, 0.75]</td></tr><tr><th>4</th><td>newton-cg</td><td>0.1</td><td>0.796226</td><td>[0.775281, 0.806818, 0.806818]</td></tr><tr><th>5</th><td>lbfgs</td><td>0.1</td><td>0.796226</td><td>[0.775281, 0.806818, 0.806818]</td></tr><tr><th>6</th><td>liblinear</td><td>0.1</td><td>0.735849</td><td>[0.719101, 0.704545, 0.784091]</td></tr><tr><th>7</th><td>newton-cg</td><td>0.5</td><td>0.784906</td><td>[0.775281, 0.772727, 0.806818]</td></tr><tr><th>8</th><td>lbfgs</td><td>0.5</td><td>0.784906</td><td>[0.775281, 0.772727, 0.806818]</td></tr><tr><th>9</th><td>liblinear</td><td>0.5</td><td>0.739623</td><td>[0.707865, 0.727273, 0.784091]</td></tr><tr><th>10</th><td>newton-cg</td><td>0.9</td><td>0.792453</td><td>[0.775281, 0.772727, 0.829545]</td></tr><tr><th>11</th><td>lbfgs</td><td>0.9</td><td>0.792453</td><td>[0.775281, 0.772727, 0.829545]</td></tr><tr><th>12</th><td>liblinear</td><td>0.9</td><td>0.754717</td><td>[0.730337, 0.738636, 0.795455]</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nThe best model from the grid search will be saved in `best_estimator_` field of your grid search training results. \n\n::: {.cell execution_count=13}\n``` {.julia .cell-code}\nbest_logistic_model = gridsearch_logistic.best_estimator_\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\nPyObject LogisticRegression(C=0.01, solver='newton-cg')\n```\n:::\n:::\n\n\n### Step 15: Repeat GridSearchCV for other models you have  {.unnumbered}\n\n- Just don't simply use the list of numbers I have used for the `hidden_layer_sizes`. It might make sense to have 1000s of nodes and many hidden layers if you are doing image processing. \n\nSometimes, you might have to do multiple grid search rounds with different hyperparameter settings to find the most optimal values.\n\n::: {.cell freeze='true' execution_count=14}\n``` {.julia .cell-code}\ngridsearch_neuralnet = GridSearchCV(MLPClassifier(),\n            Dict(:solver => [\"sgd\", \"lbfgs\", \"adam\"], \n            :hidden_layer_sizes => [(2), (20), (1,5,10), (10,10), (10,20,10)]))\nfit!(gridsearch_neuralnet, X_train, Y_train);\n\n\ngridsearch_neuralnet_results = DataFrame(gridsearch_neuralnet.grid_scores_);\nhcat(DataFrame(gridsearch_neuralnet_results.parameters),\n    gridsearch_neuralnet_results)[!,Not(:parameters)]\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<div class=\"data-frame\"><p>15 rows × 4 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>solver</th><th>hidden_layer_sizes</th><th>mean_validation_score</th><th>cv_validation_scores</th></tr><tr><th></th><th title=\"String\">String</th><th title=\"Any\">Any</th><th title=\"Float64\">Float64</th><th title=\"Vector{Float64}\">Array…</th></tr></thead><tbody><tr><th>1</th><td>sgd</td><td>2</td><td>0.562264</td><td>[0.685393, 0.306818, 0.693182]</td></tr><tr><th>2</th><td>lbfgs</td><td>2</td><td>0.690566</td><td>[0.685393, 0.693182, 0.693182]</td></tr><tr><th>3</th><td>adam</td><td>2</td><td>0.596226</td><td>[0.685393, 0.784091, 0.318182]</td></tr><tr><th>4</th><td>sgd</td><td>20</td><td>0.686792</td><td>[0.662921, 0.693182, 0.704545]</td></tr><tr><th>5</th><td>lbfgs</td><td>20</td><td>0.709434</td><td>[0.730337, 0.681818, 0.715909]</td></tr><tr><th>6</th><td>adam</td><td>20</td><td>0.683019</td><td>[0.707865, 0.704545, 0.636364]</td></tr><tr><th>7</th><td>sgd</td><td>(1, 5, 10)</td><td>0.543396</td><td>[0.674157, 0.647727, 0.306818]</td></tr><tr><th>8</th><td>lbfgs</td><td>(1, 5, 10)</td><td>0.690566</td><td>[0.685393, 0.693182, 0.693182]</td></tr><tr><th>9</th><td>adam</td><td>(1, 5, 10)</td><td>0.686792</td><td>[0.685393, 0.681818, 0.693182]</td></tr><tr><th>10</th><td>sgd</td><td>(10, 10)</td><td>0.660377</td><td>[0.662921, 0.693182, 0.625]</td></tr><tr><th>11</th><td>lbfgs</td><td>(10, 10)</td><td>0.716981</td><td>[0.707865, 0.693182, 0.75]</td></tr><tr><th>12</th><td>adam</td><td>(10, 10)</td><td>0.630189</td><td>[0.629213, 0.693182, 0.568182]</td></tr><tr><th>13</th><td>sgd</td><td>(10, 20, 10)</td><td>0.683019</td><td>[0.58427, 0.715909, 0.75]</td></tr><tr><th>14</th><td>lbfgs</td><td>(10, 20, 10)</td><td>0.750943</td><td>[0.707865, 0.772727, 0.772727]</td></tr><tr><th>15</th><td>adam</td><td>(10, 20, 10)</td><td>0.690566</td><td>[0.662921, 0.670455, 0.738636]</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=15}\n``` {.julia .cell-code}\nbest_neuralnetwork_model = gridsearch_neuralnet.best_estimator_\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\nPyObject MLPClassifier(hidden_layer_sizes=(10, 20, 10), solver='lbfgs')\n```\n:::\n:::\n\n\n### Step 16: Compare the results of competing models on test set {.unnumbered}\n\n::: {.cell execution_count=16}\n``` {.julia .cell-code}\n@sk_import metrics: accuracy_score\nY_pred_test_logistic = predict(best_logistic_model, X_test)\nlogistic_accuracy = accuracy_score(Y_test,Y_pred_test_logistic)\n\nY_pred_test_neural = predict(best_neuralnetwork_model, X_test)\nneural_accuracy = accuracy_score(Y_test,Y_pred_test_neural)\n\nmodels = [\"logistic\",\"neural\"]\nscores = [logistic_accuracy, neural_accuracy]\n# Plotting the results \n\nusing PyPlot  \nfigure()\nb = PyPlot.bar(x = models, height = scores*100);\nxlabel(\"Models\"); \nylabel(\"Accuracy Score\"); \n```\n\n::: {.cell-output .cell-output-display}\n![](e2e_files/figure-html/cell-17-output-1.png){}\n:::\n:::\n\n\nHere we have used accuracy for comparison. You can also use precision, recall, or f1 score in a similar fashion for comparing models, depending on the domain of prediction.\n\n### Step 17: Save your best model for production {.unnumbered}\n\nThe model you found to be the best performing one can be saved to the disk so that you don't have to train your models every time you restart Julia or you want to make predictions. We'll need three packages to save a `scikit-learn` model: `PyCall`, `PyCallJLD`, and `JLD`\n\n::: {.cell execution_count=17}\n``` {.julia .cell-code}\nusing PyCall, JLD, PyCallJLD\nsave(\"saved_file.jld\", \"diabetic_prediction\", best_logistic_model)\n```\n:::\n\n\n- The save function takes three arguments: the name of the file, the name you want to give to the model, the trained model.\n\nTo load a saved `scikit-learn` model, you can use the load function: \n\n::: {.cell execution_count=18}\n``` {.julia .cell-code}\nlogistic_model = load(\"saved_file.jld\", \"diabetic_prediction\")\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\nPyObject LogisticRegression(C=0.01, solver='newton-cg')\n```\n:::\n:::\n\n\nNow let's check if our loaded model is working.\n\n::: {.cell execution_count=19}\n``` {.julia .cell-code}\npredict(logistic_model, X_test)[1:4]\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\n4-element Vector{Any}:\n \"No\"\n \"No\"\n \"No\"\n \"No\"\n```\n:::\n:::\n\n\n",
    "supporting": [
      "e2e_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}